\documentclass{article}

\usepackage{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{multirow}

\title{A Latent Neural ODE-VAE for Modeling Hippocampal Population Activity on Low-Dimensional Manifolds}

\author{%
  Anonymous Author(s) \\
  Affiliation \\
  Address \\
  \texttt{anonymous@neurips.cc} \\
}

\begin{document}

\maketitle

\begin{abstract}
Neural population activity traces trajectories in a high-dimensional state space, yet accumulating evidence suggests these trajectories are confined to low-dimensional manifolds that encode both task variables and internal state. Existing manifold inference pipelines can recover geometry and explain variability, but often rely on multi-stage local models and do not impose globally smooth continuous-time dynamics. We develop a latent Neural ODE variational autoencoder (ODE-VAE) that jointly learns (i) a low-dimensional stochastic initial condition, (ii) continuous-time latent dynamics parameterized by a mixture-of-experts ODE, and (iii) a decoder back to neural activity. To better align reconstruction with temporally structured variability, our \texttt{v5} implementation adds transition-consistency regularization in observation space and a soft locally linear embedding (LLE) constraint in latent space. On synthetic random-foraging sequences, the model achieves high reconstruction accuracy (\(R^2=0.9789\)) while exhibiting seed sensitivity. On the E65 hippocampal calcium dataset, the latest \texttt{v5} baseline run attains final PCA-space reconstruction \(R^2=0.4506\) (best epoch \(R^2=0.4785\)), but performance varies substantially with preprocessing, split strategy, and evaluation space. Together, these results highlight both the promise and current fragility of end-to-end continuous-time manifold models for noisy biological recordings.
\end{abstract}

\section{Introduction}
Neural activity can be described as a point in a high-dimensional coordinate system, where each coordinate axis represents a single neuron's activity \citep{cunningham2014}. Underlying properties of the network and its inputs can confine neural trajectories to a subregion of this space, often referred to as a neural manifold \citep{cunningham2014,gallego2017}. The neural manifold has been proposed to underlie motor movements \citep{gallego2017,russo2018}, head direction cells \citep{chaudhuri2019}, and hippocampal maps of physical variables \citep{okeefe1971,frank2000,wood2000,okeefe1978}. The conceptual ideas in these studies suggest a general principle of hippocampal computation: the construction of organized maps of learned knowledge instantiated by neural manifolds \citep{tolman1948,okeefe1978,stachenfeld2017,bellmund2018,nieh2021}.

Nonlinear dimensionality reduction has demonstrated that neural population activity can often be described by 4--6 latent variables, suggesting that activity is constrained to a low-dimensional neural manifold that displays a geometric representation of both physical and abstract variables \citep{low2018,chaudhuri2019,nieh2021}. Existing approaches are limited to multi-stage machine-learning pipelines, using forest-based transition models (with probabilistic principal component analysis in decision-tree leaves) to define distances between population states, which are then embedded into a low-dimensional manifold and mapped back to neural activity for reconstruction \citep{low2018,tipping1999,breiman2001,tenenbaum2000,yu2009}. This piecewise approach partitions state space and models dynamics locally, hence lacking explicit enforcement of globally smooth latent dynamics and can exhibit saturation of reconstruction decoding performance with low-dimensional embeddings \citep{low2018}. Thus, we hypothesize that generative deep learning models offer a complementary framework: neural network architectures can be trained directly on biological neural population recordings to jointly learn low-dimensional latent coordinates, their temporal evolution, and the mapping back to neural activity \citep{kingma2014,chen2018,rubanova2019}.

In this paper, we propose a novel approach to modeling the neural manifold by constructing a Neural Ordinary Differential Equation variational autoencoder (ODE-VAE): a deep generative model that (i) encodes high-dimensional population activity into a low-dimensional latent state, (ii) models the evolution of that latent state as a continuous-time dynamical system parameterized by a neural ODE, and (iii) decodes the resulting latent trajectory back into neural activity \citep{kingma2014,chen2018,rubanova2019}. By training the encoder, dynamics, and decoder end-to-end under a variational objective, this approach aims to capture nonlinear manifold structure while imposing smooth temporal dynamics.
Our \texttt{v5} implementation introduces mixture-of-experts latent dynamics and adds two regularizers inspired by manifold inference---transition-consistency in observation space and a soft locally linear embedding (LLE) constraint in latent space \citep{low2018,saul2003}. We evaluate this family on synthetic and hippocampal calcium datasets and analyze the sensitivity of performance to preprocessing and evaluation choices.

\paragraph{Contributions.}
\begin{itemize}
\item We formalize an ODE-VAE for trialized population sequences with mixture-of-experts latent dynamics and explicit geometric regularizers.
\item We instantiate this formulation in a versioned codebase (\texttt{v1--v6}) and present \texttt{v5} as the primary model with transition and soft-LLE regularization.
\item We provide a reproducible evaluation on synthetic and hippocampal calcium datasets and identify protocol factors that strongly affect reconstruction metrics.
\end{itemize}

\section{Related Work}
Our approach lies at the intersection of manifold-based neuroscience and latent dynamical systems. In hippocampus, the cognitive map framework and subsequent experimental work motivate geometric organization of population codes \citep{tolman1948,okeefe1971,okeefe1978,eichenbaum2014}, including abstract and non-spatial representations \citep{aronov2017,tavares2015,constantinescu2016,schuck2019,park2020,nieh2021}. Beyond classical place coding, hippocampal population activity reflects trajectory and sequential organization \citep{frank2000,pastalkova2008,macdonald2011,taxidis2020}, episodic variables at shared locations \citep{wood2000,gill2011,mckenzie2014}, and multimodal/task variables such as odor and taste \citep{eichenbaum1987,herzog2019}. Manifold inference methods can recover low-dimensional structure and explain structured variability beyond measured task variables \citep{low2018,chaudhuri2019,rubin2019}.

In machine learning, variational autoencoders \citep{kingma2014} and neural ODEs \citep{chen2018} provide a principled framework for continuous-time latent-variable modeling. Latent ODEs extend this idea to irregularly sampled sequences \citep{rubanova2019}. We adopt this framework but tailor the encoder, evaluation protocol, and regularization to the neuroscience setting, emphasizing trialized sequences, explicit geometric constraints, and comparisons to MIND-style evaluation pipelines \citep{low2018}. For calcium imaging recordings, related methodological work has emphasized motion correction and demixing/denoising \citep{pnevmatikakis2016,normcorre2017}, highlighting the importance of preprocessing choices when evaluating reconstruction metrics.

\section{Problem Setup and Data}
We study trialized population activity sequences. Let \(y_b(t_\ell)\in\mathbb{R}^{N}\) denote the raw activity of \(N\) simultaneously recorded units/ROIs on trial \(b\in\{1,\dots,B\}\) at resampled time \(t_\ell\), where \(\ell\in\{1,\dots,L\}\) indexes a fixed-length grid. We write \(Y_b\in\mathbb{R}^{L\times N}\) for the stacked sequence.

\paragraph{Observation space.}
In \texttt{v5}, the model is trained on a PCA-projected representation of activity. Let \(x_b(t_\ell)\in\mathbb{R}^{K}\) be the \(K\)-dimensional PCA coordinate at time \(t_\ell\), and let \(X_b\in\mathbb{R}^{L\times K}\) be the corresponding trial sequence. Unless otherwise stated, all losses and reported \(R^2\) values for \texttt{v5} are computed in this PCA space. For some evaluations (e.g., the optional sweep path), reconstructions are mapped back to raw ROI space via inverse PCA and de-normalization.

\paragraph{Time grid.}
Trials are resampled to a common duration and the time vector is normalized to \([0,1]\); we denote the resulting grid by \(0=t_1<\cdots<t_L=1\). The latent dimension is denoted by \(D\).

\paragraph{E65 dataset.}
We use the Schottdorf Lab E65 dataset (\texttt{E65\_data.npz}), containing calcium activity (\(\Delta F/F\)) from \(N=375\) ROIs over \(T=7434\) frames, along with trial IDs, timestamps, and aligned behavioral covariates. In the \texttt{v5} preprocessing path: (i) PCA is fit to the full recording and retains 95\% variance, producing \(K=129\) components; (ii) frames are grouped by trial, the first 10 trials are dropped, and each trial is linearly interpolated to a fixed length \(L=120\) (\texttt{trial\_len\_s=12}, \texttt{fps=10}); (iii) the time vector is normalized to \([0,1]\); (iv) each PCA component is standardized over time (session-level z-score) and an optional per-trial baseline is removed by subtracting the mean of the first 5 resampled bins. After filtering, 180 trials are available; default validation holds out the last 3 trials (train 177 / val 3). For efficiency, the default configuration further subsamples to 100 sequences via greedy landmark selection.

\paragraph{Synthetic benchmark.}
We additionally evaluate on \texttt{synthetic\_rat\_data.npz} (4000 frames, 300 neurons, 20 trials), which provides a controlled benchmark for recoverability of smooth low-dimensional dynamics.

\section{Model: Latent Neural ODE-VAE}
\subsection{Stochastic encoder}
For each trial, the encoder uses only \(x_b(t_1)\) and outputs a diagonal Gaussian posterior on the latent initial state:
\begin{equation}
q_\phi(z_{0,b} \mid x_b(t_1)) = \mathcal{N}\!\left(\mu_b,\operatorname{diag}(\sigma_b^2)\right),
\end{equation}
with reparameterization
\begin{equation}
z_{0,b} = \mu_b + \sigma_b \odot \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I).
\end{equation}
Here \(z_{0,b}\in\mathbb{R}^D\), \(\mu_b\in\mathbb{R}^D\), \(\sigma_b\in\mathbb{R}^D_{>0}\), and \(\odot\) denotes elementwise multiplication.

\subsection{Continuous-time latent dynamics}
Latent trajectories are generated by a neural ODE:
\begin{equation}
\frac{dz_b(t)}{dt} = f_\theta(z_b(t), t), \qquad z_b(t_1)=z_{0,b}.
\end{equation}
In \texttt{v5}, \(f_\theta\) is a mixture of experts:
\begin{equation}
f_\theta(z,t) = \sum_{e=1}^{E} \pi_e(z) f_e(z), \qquad
\pi(z)=\operatorname{softmax}(g(z)),
\end{equation}
with \(E=4\) latent experts by default and Dormand--Prince integration (\texttt{dopri5}).
Each expert \(f_e:\mathbb{R}^D\to\mathbb{R}^D\) is an MLP and \(\pi_e(z)\in[0,1]\) are gating weights satisfying \(\sum_e \pi_e(z)=1\).

\subsection{Decoder family}
A decoder maps latent states back to observations:
\begin{equation}
\hat{x}_b(t_\ell)=g_\psi(z_b(t_\ell)).
\end{equation}
The codebase supports MLP, neuron-aware, local-attention, and MoE decoders; \texttt{v5} default is MoE decoder with 8 decoder experts.
In all cases, \(g_\psi:\mathbb{R}^D\to\mathbb{R}^K\) outputs the mean of a factorized Gaussian observation model in PCA space.

\section{Training Objective and Regularization}
We optimize a variational objective with auxiliary regularizers. Under a Gaussian observation model \(p_\psi(x_b(t_\ell)\mid z_b(t_\ell))=\mathcal{N}(g_\psi(z_b(t_\ell)),\sigma^2 I)\) with fixed \(\sigma^2\), maximizing the ELBO corresponds (up to constants and a scale factor) to minimizing mean-squared reconstruction error plus a KL penalty.

The base objective combines reconstruction and KL terms:
\begin{equation}
\mathcal{L}_{\text{base}} = \mathcal{L}_{\text{rec}} + \beta\,\mathcal{L}_{\text{KL}},
\end{equation}
where
\begin{equation}
\mathcal{L}_{\text{rec}} = \frac{1}{B\,L\,K}\sum_{b=1}^{B}\sum_{\ell=1}^{L}\|\hat{x}_b(t_\ell)-x_b(t_\ell)\|_2^2,
\end{equation}
\begin{equation}
\mathcal{L}_{\text{KL}} = \frac{1}{B}\sum_b D_{\text{KL}}\big(q_\phi(z_{0,b}\mid x_b(t_1))\,\|\,\mathcal{N}(0,I)\big).
\end{equation}
Equivalently, the (negative) ELBO per trial is
\begin{equation}
\mathcal{L}_{\text{ELBO}} =
-\mathbb{E}_{q_\phi(z_{0,b}\mid x_b(t_1))}\Big[\sum_{\ell=1}^{L}\log p_\psi(x_b(t_\ell)\mid z_b(t_\ell))\Big]
+\beta\,D_{\mathrm{KL}}\big(q_\phi(z_{0,b}\mid x_b(t_1))\,\|\,p(z_{0,b})\big),
\end{equation}
with prior \(p(z_{0,b})=\mathcal{N}(0,I)\). In practice, the code uses a single Monte Carlo sample of \(z_{0,b}\) per trial and minibatch.

\paragraph{Smoothness regularization.}
\begin{equation}
\mathcal{L}_{\text{smooth}} = \frac{1}{B(L-1)D}\sum_{b,\ell}
\left\|\frac{z_b(t_{\ell+1})-z_b(t_\ell)}{t_{\ell+1}-t_\ell}\right\|_2^2.
\end{equation}

\paragraph{Transition-aware regularization (\texttt{v5}).}
\begin{equation}
\mathcal{L}_{\text{trans}} = \frac{1}{B(L-1)K}\sum_{b,\ell}
\left\|\big(\hat{x}_b(t_{\ell+1})-\hat{x}_b(t_\ell)\big)-\big(x_b(t_{\ell+1})-x_b(t_\ell)\big)\right\|_2^2.
\end{equation}
This term is linearly warmed up for the first 30 epochs.

\paragraph{Soft LLE latent regularization (\texttt{v5}).}
For flattened latent points \(\{z_i\}_{i=1}^M\subset\mathbb{R}^D\), with \(k\)-NN set \(\mathcal{N}_k(i)\), we add a soft locally linear embedding penalty \citep{saul2003}:
\begin{equation}
\mathcal{L}_{\text{LLE}} = \frac{1}{M}\sum_{i=1}^{M}
\left\|z_i-\sum_{j\in\mathcal{N}_k(i)}w_{ij}z_j\right\|_2^2,
\quad
w_{ij}\propto\exp\!\left(-\frac{\|z_i-z_j\|_2}{\tau}\right).
\end{equation}
Default parameters: \(k=8\), \(M\le 256\), \(\tau=0.1\).

\paragraph{Total loss.}
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{rec}} + \beta_t\mathcal{L}_{\text{KL}} +
\lambda_{\text{smooth}}\mathcal{L}_{\text{smooth}} +
\lambda_{\text{trans},t}\mathcal{L}_{\text{trans}} +
\lambda_{\text{LLE}}\mathcal{L}_{\text{LLE}}.
\end{equation}
The KL coefficient \(\beta_t\) is warmed up over 30 epochs to a final value \(\beta=0.02\).

\section{Experimental Protocol}
\subsection{Configurations}
Main \texttt{v5} settings from \texttt{config.txt}: latent dimension \(D=5\) (with a sweep to \(D=8\)), batch size 8, 150 epochs, Adam optimizer (learning rate 0.002, weight decay \(10^{-5}\)), \(\lambda_{\text{smooth}}\in\{5\times10^{-4},2\times10^{-4}\}\), \(\lambda_{\text{trans}}=0.01\) with a 30-epoch warmup, \(\lambda_{\text{LLE}}=0.01\), landmark count 100, and baseline correction enabled.

\paragraph{Implementation details.}
The encoder is an MLP with hidden widths 512--256--128. The latent vector field uses \(E=4\) experts with hidden width 128 and a learned gating network; derivatives are layer-normalized for stability. For reconstruction, the default \texttt{v5} decoder is a mixture-of-experts network with 8 decoder experts and hidden width 256. Latent dynamics are integrated with Dormand--Prince (\texttt{dopri5}) using tolerances \texttt{rtol=1e-3} and \texttt{atol=1e-4}. Gradients are clipped to max norm 1.0.

\subsection{Metrics}
The primary training metric is coefficient of determination,
\begin{equation}
R^2 = 1-\frac{\sum_{b,\ell}\|x_b(t_\ell)-\hat{x}_b(t_\ell)\|_2^2}{\sum_{b,\ell}\|x_b(t_\ell)-\bar{x}\|_2^2},
\end{equation}
where \(\bar{x}=\frac{1}{B\,L}\sum_{b,\ell} x_b(t_\ell)\) denotes the mean activity vector across all validation entries in the evaluation space.
Our implementation supports two evaluation styles: (i) PCA-space \(R^2\) and (ii) strict raw-neuron-space \(R^2\) via inverse PCA and de-normalization. We report values as saved in the run metadata for each experiment.

\section{Results}
\subsection{Synthetic benchmark: high ceiling with seed sensitivity}
Table~\ref{tab:synthetic} summarizes a five-seed sweep on synthetic data. The best seed reaches \(R^2=0.9789\). Excluding the divergent run, the mean performance is \(R^2=0.828\pm 0.141\) (std. dev.), indicating sensitivity to initialization even in a controlled setting.

\begin{table}[t]
\centering
\caption{Synthetic random-foraging benchmark from \texttt{seed\_sweep\_results.txt}.}
\label{tab:synthetic}
\begin{tabular}{lcc}
\toprule
Seed & Final \(R^2\) & Best validation loss \\
\midrule
1 & 0.9789 & 0.06642 \\
42 & 0.6757 & 0.40521 \\
1337 & 0.9116 & 0.16298 \\
2025 & 0.7467 & 0.31145 \\
777 & 0.0000 (NaN collapse) & $\infty$ \\
\midrule
Mean (all seeds) & 0.6626 & -- \\
Mean (non-collapsed seeds) & 0.8282 & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{E65 hippocampal data: mixed performance across variants}
Table~\ref{tab:e65} reports available E65 run metrics for the ODE-VAE family. The most recent run in the repository (\texttt{2026-02-20\_172318\_6880b5b}) achieves a final PCA-space \(R^2=0.4506\) with best-epoch \(R^2=0.4785\). An earlier archived ODE-VAE run reports \(R^2=0.4368\). In contrast, \texttt{v5} sweep artifacts are substantially lower (best-epoch \(R^2\le 0.0569\), negative final \(R^2\) in three runs), while a no-PCA \texttt{v6} run records \(R^2=0.0956\). These discrepancies are consistent with the codebase supporting multiple evaluation spaces and split protocols; therefore, comparisons across runs should be interpreted cautiously unless the evaluation definition is matched.

\begin{table}[t]
\centering
\caption{E65 reconstruction metrics extracted from saved run artifacts. Sweep runs report both best-epoch and final \(R^2\) as logged during training.}
\label{tab:e65}
\begin{tabular}{lccccc}
\toprule
Variant & Space & \(D\) & \(\lambda_{\text{smooth}}\) & Best \(R^2\) & Final \(R^2\) \\
\midrule
latest \texttt{v5} baseline & PCA & 5 & \(5\times 10^{-4}\) & 0.4785 & 0.4506 \\
archived ODE-VAE & PCA & -- & -- & -- & 0.4368 \\
\texttt{v5} sweep & PCA & 5 & \(5\times 10^{-4}\) & 0.0354 & -0.2391 \\
\texttt{v5} sweep & PCA & 5 & \(2\times 10^{-4}\) & 0.0353 & -0.1382 \\
\texttt{v5} sweep & PCA & 8 & \(5\times 10^{-4}\) & 0.0569 & -0.2022 \\
\texttt{v6} (no PCA) & raw & -- & -- & -- & 0.0956 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Manifold interpretability}
The codebase saves latent manifold projections (MDS) and reconstruction diagnostics for each run. Figure~\ref{fig:manifold} shows an example latent trajectory embedding from the trained model artifacts.

\begin{figure}[t]
\centering
\includegraphics[width=0.62\linewidth]{neuroscience/src/pt_files/latent_manifold_mds.png}
\caption{Latent manifold embedding produced by the ODE-VAE analysis pipeline.}
\label{fig:manifold}
\end{figure}

\section{Discussion}
The model captures the intended inductive bias: low-dimensional continuous latent trajectories with explicit geometric regularization. On synthetic data, this bias is highly effective. On real E65 recordings, however, results are sensitive to implementation and evaluation choices.

Three factors emerge from the saved run artifacts:
\begin{enumerate}
\item \textbf{Metric-space mismatch.} PCA-space training can look favorable while strict raw-space \(R^2\) may degrade. This is particularly salient when comparing to MIND-style evaluations, which reconstruct back to neuron space (via inverse PCA) before scoring \citep{low2018}.
\item \textbf{Data-efficiency tradeoff.} Landmark subsampling (100 selected sequences from 180 usable trials) accelerates training but may reduce generalization. In MIND, landmarks primarily support graph construction and embedding; the learned mapping is then applied to all eligible time points \citep{low2018}.
\item \textbf{Optimization stability.} Strong regularization with small validation sets (3 trials) and stiff latent dynamics can produce unstable or negative final \(R^2\), despite early high points.
\end{enumerate}

These observations suggest that future gains likely require protocol-level changes in addition to architectural changes: larger and randomized holdout splits, early stopping on a stable cross-validated objective, trial-level (not frame-level) landmark selection, and direct raw-space reconstruction losses.

\section{Future Work}
A central motivation of this project is to connect end-to-end continuous-time latent dynamical modeling with the multi-stage manifold inference pipeline used in MIND \citep{low2018}. Our current codebase already adopts several MIND-inspired components (global PCA preprocessing, greedy landmark selection for visualization, and MDS-based manifold plots), but the modeling philosophy differs: MIND estimates a graph of transition structure via a PPCA regression forest and learns explicit local mappings between ambient activity and manifold coordinates \citep{low2018,tipping1999,breiman2001}, whereas the ODE-VAE learns a single global generative model (encoder + latent dynamics + decoder) by optimizing a reconstruction objective \citep{kingma2014,chen2018,rubanova2019}.
Below we outline concrete directions to tighten this connection and improve robustness on calcium recordings.

\subsection{Match MIND-style evaluation protocols and metrics}
Many apparent discrepancies across saved E65 runs are consistent with evaluation-definition mismatch. In the MIND Matlab cross-validation script, trials are split randomly (e.g., 90/10), reconstruction is scored in the original neuron space after mapping back through inverse PCA, and performance is visualized both as an overall score and as per-trial dots \citep{low2018}. Aligning our training and reporting with this protocol would make comparisons substantially more interpretable.
Concretely, we plan to (i) report both Pearson correlation on vectorized activity blocks,
\begin{equation}
r = \mathrm{corr}\!\left(\mathrm{vec}(Y_{\mathrm{test}}),\mathrm{vec}(\hat{Y}_{\mathrm{test}})\right),
\end{equation}
and variance-explained \(R^2\) under repeated random trial splits, and (ii) include held-out neuron evaluation where latents are inferred from a subset of neurons and used to predict excluded neurons, mirroring the ``cell prediction'' analyses in MIND \citep{low2018}. This will also require revisiting the current practice of validating on the final 3 trials, which can conflate generalization with drift.

\subsection{Use landmarks for geometry, not for shrinking the training set}
In MIND, landmarks are an efficiency device for graph construction and embedding; the learned mapping is then applied to all eligible time points \citep{low2018}. In contrast, the default \texttt{v5} configuration further subsamples the dataset down to 100 landmarked sequences (from 180 trials), which likely increases estimator variance and can bias which trials are emphasized during training.
A straightforward next step is to train the ODE-VAE on all trials/time points and reserve landmark selection for: (i) visualization, (ii) neighbor graph construction for local regularizers, and (iii) lightweight geometric diagnostics (e.g., random-walk distance embeddings). This change should directly improve stability without changing the model class.

\subsection{Hybrid decoders: combine global reconstruction with MIND-like local mappings}
The MIND pipeline learns mappings between ambient PCA space and manifold coordinates using locally weighted methods (e.g., LLE regression) \citep{saul2003,low2018}. This provides a natural mechanism to capture sharp, local irregularities that global regressors may smooth out. Our current \texttt{v5} decoders are global function approximators (MLP/MoE), which can yield good coarse reconstructions but may miss neuron-specific transients.
An appealing hybrid is a global decoder plus a local residual term defined over nearby latent states,
\begin{equation}
\hat{x}(t) = g_\psi(z(t)) + \sum_{j\in\mathcal{N}_k(z(t))} \alpha_j(z(t))\,r_j,
\end{equation}
where \(\mathcal{N}_k(\cdot)\) are neighbors in latent space (or in a MIND-style random-walk metric), \(r_j\in\mathbb{R}^K\) are learned prototype residuals, and \(\alpha_j\) are normalized weights (e.g., softmax over distances). This would preserve the interpretability and global smoothness of the ODE while injecting the kind of local adaptivity that MIND's mapping stage provides.

\subsection{Optimize and score in raw neuron space (with PCA as an internal linear layer)}
Several E65 runs in this repository train and score in different spaces (PCA vs raw ROI), making \(R^2\) values hard to compare. MIND keeps PCA primarily as a compression step but reconstructs back to the original activity space before computing reconstruction scores \citep{low2018}. A direct analogue for the ODE-VAE is to keep a fixed (or lightly fine-tuned) PCA projection for computational efficiency, but decode back to raw ROI space and compute the main reconstruction loss on \(y_b(t)\in\mathbb{R}^N\).
One implementation is to parameterize a raw-space decoder as
\(\hat{y}(t) = W_{\mathrm{PCA}}^\top \hat{x}(t) + \mu\),
using the PCA loading matrix \(W_{\mathrm{PCA}}\) and mean \(\mu\) from preprocessing, and to define \(\mathcal{L}_{\mathrm{rec}}\) in raw space. This would more closely match the scientific question---reconstructing neural activity---and reduce the chance that good PCA-space fits hide biologically relevant errors.

\subsection{Make latent dynamics probabilistic to better match MIND transition structure}
MIND estimates transition structure via a probabilistic model of next-step activity (a PPCA regression forest) and then derives a random-walk geometry from transition probabilities \citep{low2018,tipping1999,breiman2001}. Our latent ODE is deterministic given \(z_{0,b}\), which can be brittle when real data exhibit unmodeled inputs, nonstationarities, or observation noise.
A natural extension is to introduce process noise (Neural SDEs) or discrete-time stochastic residuals,
\(z(t_{\ell+1}) = z(t_\ell) + \int_{t_\ell}^{t_{\ell+1}} f_\theta(z(t),t)\,dt + \eta_\ell\),
which can absorb variability not explained by the initial condition while retaining smooth latent structure. This direction also creates a clearer conceptual bridge between ODE-based dynamics and MIND's transition-probability graph.

\subsection{Geometry-aware objectives beyond soft-LLE}
Our current soft-LLE penalty encourages local linearity in the learned latent point cloud, but it does not directly use transition structure. The MIND code constructs local distances from transition probabilities (e.g., \(d_{ij}\propto \sqrt{-\log p_{ij}}\)) and then computes geodesic distances on the resulting graph before embedding \citep{low2018}. This is conceptually related to geodesic-distance embeddings in nonlinear dimensionality reduction \citep{tenenbaum2000}. A promising direction is to import this idea as a regularizer: estimate a transition graph among landmarked latent points, compute a random-walk geodesic distance matrix, and penalize distortions between these distances and Euclidean distances in the latent embedding. Such a constraint could encourage the latent representation to respect the sequential structure that MIND leverages, while still permitting an end-to-end generative model.

\section{Limitations and Reproducibility}
This study is bounded by the available run artifacts and inherits version-specific logging differences. In particular, some run files report ``best'' and ``final'' \(R^2\) under different conditions, and not all checkpoints include identical metadata fields. We therefore report values exactly as saved in each artifact path. The implementation also exhibits training fragility (including occasional NaN divergence), which should be addressed before drawing definitive biological conclusions.

\section{Conclusion}
We presented a mathematically grounded latent Neural ODE-VAE framework for neural manifold modeling and analyzed a sequence of model variants (\texttt{v1--v6}), with \texttt{v5} as the primary model. The method can recover smooth low-dimensional dynamics and high synthetic reconstruction quality, but real-data performance remains sensitive to preprocessing and evaluation protocol. This work provides a formal foundation and concrete directions for improving robustness of ODE-VAE manifold modeling for neuroscience.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
