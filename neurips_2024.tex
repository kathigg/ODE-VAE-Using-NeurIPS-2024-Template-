\documentclass{article}

\usepackage{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{multirow}

\title{A Latent Neural ODE-VAE for Modeling Hippocampal Population Activity on Low-Dimensional Manifolds}

\author{%
  Anonymous Author(s) \\
  Affiliation \\
  Address \\
  \texttt{anonymous@neurips.cc} \\
}

\begin{document}

\maketitle

\begin{abstract}
Neural activity can be represented as a trajectory in a high-dimensional space where each axis corresponds to one neuron. A central question in systems neuroscience is whether these trajectories are constrained to low-dimensional manifolds that jointly encode external variables and trial-to-trial internal variability. Building on this question, we develop and analyze a latent Neural ODE variational autoencoder (ODE-VAE) for calcium population recordings. The model encodes trial onset activity into a stochastic latent initial condition, evolves it through a continuous-time mixture-of-experts neural ODE, and decodes latent trajectories back to neural activity. Relative to earlier ODE-VAE versions, \texttt{v5} adds transition-aware reconstruction regularization and a soft locally linear embedding (LLE) latent constraint. Using repository-grounded experiments, we report strong reconstruction on synthetic random-foraging data (best seed \(R^2=0.9789\)) and mixed outcomes on the E65 hippocampal dataset, including an archived ODE-VAE run at \(R^2=0.4368\), strict raw-space \texttt{v5} sweeps with best-epoch \(R^2\le 0.0569\), and a no-PCA \texttt{v6} variant at \(R^2=0.0956\). These results show that continuous-time latent manifold models can capture structured dynamics, but evaluation protocol, preprocessing, and regularization balance strongly affect performance on real neural data.
\end{abstract}

\section{Introduction}
Neural activity can be described as a point in a high-dimensional coordinate system, where each coordinate axis represents a single neuron's activity. Prior work shows that population trajectories often occupy low-dimensional manifolds, in motor cortex, hippocampus, and entorhinal circuits \citep{gallego2017,russo2018,okeefe1971,aronov2017,nieh2021}. This geometric view supports the broader ``cognitive map'' hypothesis: neural circuits organize both physical and abstract knowledge into structured latent spaces \citep{tolman1948,okeefe1978,stachenfeld2017,bellmund2018}.

Recent manifold analyses of hippocampal recordings report intrinsic dimensionalities around 4--6 and show that manifold geometry can explain both task variables and trial-level variability \citep{low2018,chaudhuri2019,nieh2021}. However, many pipelines are multi-stage: fit local transition models, build pairwise distances, embed points, and separately map back to activity \citep{low2018,tenenbaum2000,yu2009}. This decomposition gives useful geometry but does not explicitly enforce globally smooth continuous-time dynamics in a single end-to-end objective.

This paper develops a continuous-time generative alternative tailored to the repository's ODE-VAE codebase. Starting from an initial latent ODE-VAE, we focus on \texttt{v5}, which adds (i) a mixture-of-experts (MoE) latent vector field, (ii) transition-aware reconstruction regularization, and (iii) soft LLE regularization in latent space. We evaluate this family on synthetic and real hippocampal calcium data and document where the method works and where it fails.

\paragraph{Contributions.}
\begin{itemize}
\item We present a mathematically explicit latent Neural ODE-VAE formulation for trial-based neural population sequences, including MoE latent dynamics and local-geometric regularizers.
\item We instantiate this formulation using the repository's \texttt{v1--v6} implementations, with \texttt{v5} as the main model and \texttt{v6} as a no-PCA extension.
\item We provide artifact-grounded empirical analysis on synthetic and E65 datasets, including seed sensitivity and configuration-dependent instability.
\item We identify concrete protocol factors (split strategy, landmark subsampling, and metric space) that materially influence reported \(R^2\).
\end{itemize}

\section{Related Work}
Our approach sits at the intersection of hippocampal manifold neuroscience and deep latent dynamical systems. In neuroscience, place cells and cognitive map theories motivate low-dimensional geometric structure in population activity \citep{okeefe1971,okeefe1978,eichenbaum2014}. More recent work extends these ideas to abstract and non-spatial variables \citep{constantinescu2016,schuck2019,park2020,aronov2017,nieh2021}. Population-level manifold analyses in hippocampus and other circuits show that latent geometry can explain behavior and internal state variability \citep{low2018,chaudhuri2019,gallego2017}.

In machine learning, variational autoencoders \citep{kingma2014} and neural ODEs \citep{chen2018} enable continuous-time latent-variable models, including latent ODEs for irregular trajectories \citep{rubanova2019}. Our model adopts this framework but targets neuroscientific interpretability: trialized sequences, manifold visualizations, and explicit regularizers for smoothness, transition consistency, and local linearity.

\section{Problem Setup and Data}
Let \(x_b(t_\ell) \in \mathbb{R}^{N}\) denote population activity for trial \(b\in\{1,\dots,B\}\) at resampled time index \(\ell\in\{1,\dots,L\}\).

\paragraph{E65 dataset.}
The repository's main real dataset is \texttt{E65\_data.npz}, containing calcium activity \((N=375,\ T=7434)\), trial IDs, timestamps, and aligned behavioral covariates. In the \texttt{v5} preprocessing path: (i) PCA retains 95\% variance, producing \(K=129\) components; (ii) trials are resampled to \(L=120\) bins using \texttt{trial\_len\_s=12} and \texttt{fps=10}; (iii) first 10 trials are dropped, yielding 180 usable trials; (iv) default validation holds out 3 trials (train 177 / val 3).

\paragraph{Synthetic benchmark.}
The repository also includes \texttt{synthetic\_rat\_data.npz} (4000 frames, 300 neurons, 20 trials), used to test recoverability of smooth low-dimensional dynamics under controlled conditions.

\section{Model: Latent Neural ODE-VAE}
\subsection{Stochastic encoder}
For each trial, the encoder uses only \(x_b(t_1)\) and outputs a diagonal Gaussian posterior on the latent initial state:
\begin{equation}
q_\phi(z_{0,b} \mid x_b(t_1)) = \mathcal{N}\!\left(\mu_b,\operatorname{diag}(\sigma_b^2)\right),
\end{equation}
with reparameterization
\begin{equation}
z_{0,b} = \mu_b + \sigma_b \odot \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I).
\end{equation}

\subsection{Continuous-time latent dynamics}
Latent trajectories are generated by a neural ODE:
\begin{equation}
\frac{dz_b(t)}{dt} = f_\theta(z_b(t), t), \qquad z_b(t_1)=z_{0,b}.
\end{equation}
In \texttt{v5}, \(f_\theta\) is a mixture of experts:
\begin{equation}
f_\theta(z,t) = \sum_{e=1}^{E} \pi_e(z) f_e(z), \qquad
\pi(z)=\operatorname{softmax}(g(z)),
\end{equation}
with \(E=4\) latent experts by default and Dormand--Prince integration (\texttt{dopri5}).

\subsection{Decoder family}
A decoder maps latent states back to observations:
\begin{equation}
\hat{x}_b(t_\ell)=g_\psi(z_b(t_\ell)).
\end{equation}
The codebase supports MLP, neuron-aware, local-attention, and MoE decoders; \texttt{v5} default is MoE decoder with 8 decoder experts.

\section{Training Objective and Regularization}
The base objective combines reconstruction and KL terms:
\begin{equation}
\mathcal{L}_{\text{base}} = \mathcal{L}_{\text{rec}} + \beta\,\mathcal{L}_{\text{KL}},
\end{equation}
where
\begin{equation}
\mathcal{L}_{\text{rec}} = \frac{1}{BLK}\sum_{b,\ell}\|\hat{x}_b(t_\ell)-x_b(t_\ell)\|_2^2,
\end{equation}
\begin{equation}
\mathcal{L}_{\text{KL}} = \frac{1}{B}\sum_b D_{\text{KL}}\big(q_\phi(z_{0,b}\mid x_b(t_1))\,\|\,\mathcal{N}(0,I)\big).
\end{equation}

\paragraph{Smoothness regularization.}
\begin{equation}
\mathcal{L}_{\text{smooth}} = \frac{1}{B(L-1)D}\sum_{b,\ell}
\left\|\frac{z_b(t_{\ell+1})-z_b(t_\ell)}{t_{\ell+1}-t_\ell}\right\|_2^2.
\end{equation}

\paragraph{Transition-aware regularization (\texttt{v5}).}
\begin{equation}
\mathcal{L}_{\text{trans}} = \frac{1}{B(L-1)K}\sum_{b,\ell}
\left\|\big(\hat{x}_b(t_{\ell+1})-\hat{x}_b(t_\ell)\big)-\big(x_b(t_{\ell+1})-x_b(t_\ell)\big)\right\|_2^2.
\end{equation}
This term is linearly warmed up for the first 30 epochs.

\paragraph{Soft LLE latent regularization (\texttt{v5}).}
For flattened latent points \(\{z_i\}_{i=1}^M\), with \(k\)-NN set \(\mathcal{N}_k(i)\):
\begin{equation}
\mathcal{L}_{\text{LLE}} = \frac{1}{M}\sum_{i=1}^{M}
\left\|z_i-\sum_{j\in\mathcal{N}_k(i)}w_{ij}z_j\right\|_2^2,
\quad
w_{ij}\propto\exp\!\left(-\frac{\|z_i-z_j\|_2}{\tau}\right).
\end{equation}
Default parameters: \(k=8\), \(M\le 256\), \(\tau=0.1\).

\paragraph{Total loss.}
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{rec}} + \beta_t\mathcal{L}_{\text{KL}} +
\lambda_{\text{smooth}}\mathcal{L}_{\text{smooth}} +
\lambda_{\text{trans},t}\mathcal{L}_{\text{trans}} +
\lambda_{\text{LLE}}\mathcal{L}_{\text{LLE}}.
\end{equation}
The KL coefficient \(\beta_t\) is warmed up over 30 epochs to a final value \(\beta=0.02\).

\section{Experimental Protocol}
\subsection{Configurations}
Main \texttt{v5} settings from \texttt{config.txt}: latent dimension 5 (with sweep to 8), batch size 8, 150 epochs, Adam optimizer (learning rate 0.002, weight decay \(10^{-5}\)), \(\lambda_{\text{smooth}}\in\{5\times10^{-4},2\times10^{-4}\}\), \(\lambda_{\text{trans}}=0.01\), \(\lambda_{\text{LLE}}=0.01\), landmark count 100, and baseline correction enabled.

\subsection{Metrics}
The primary training metric is coefficient of determination,
\begin{equation}
R^2 = 1-\frac{\sum (X-\hat{X})^2}{\sum (X-\bar{X})^2}.
\end{equation}
The repository contains two evaluation styles: (i) PCA-space \(R^2\) and (ii) strict raw-neuron-space \(R^2\) via inverse PCA and de-normalization. We report artifact values as stored in each run metadata file.

\section{Results}
\subsection{Synthetic benchmark: high ceiling with seed sensitivity}
Table~\ref{tab:synthetic} summarizes the repository seed sweep on synthetic data. Best seed reaches \(R^2=0.9789\), but one seed collapses to NaN/0.0, indicating optimizer sensitivity.

\begin{table}[t]
\centering
\caption{Synthetic random-foraging benchmark from \texttt{seed\_sweep\_results.txt}.}
\label{tab:synthetic}
\begin{tabular}{lcc}
\toprule
Seed & Final \(R^2\) & Best validation loss \\
\midrule
1 & 0.9789 & 0.06642 \\
42 & 0.6757 & 0.40521 \\
1337 & 0.9116 & 0.16298 \\
2025 & 0.7467 & 0.31145 \\
777 & 0.0000 (NaN collapse) & $\infty$ \\
\midrule
Mean (all seeds) & 0.6626 & -- \\
Mean (non-collapsed seeds) & 0.8282 & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{E65 hippocampal data: mixed performance across variants}
Table~\ref{tab:e65} reports all E65 artifact metrics found in the repository for the ODE-VAE family. An archived ODE-VAE run reports \(R^2=0.4368\). Strict raw-space \texttt{v5} sweep artifacts are substantially lower (best-epoch \(R^2\le 0.0569\), negative final \(R^2\) in three runs), while \texttt{v6} (no PCA) records \(R^2=0.0956\).

\begin{table}[t]
\centering
\caption{E65 run artifacts from saved metadata/checkpoints.}
\label{tab:e65}
\begin{tabular}{lccc}
\toprule
Artifact (path) & Variant summary & Best reported \(R^2\) & Final reported \(R^2\) \\
\midrule
\texttt{runs/ode\_vae\_E65/run\_metadata.json} & archived ODE-VAE & -- & 0.4368 \\
\texttt{runs/ode\_vae\_E65/ld5\_b0.02\_ls0.0005/run\_metadata.json} & \texttt{v5}, \(d=5\) & 0.0354 & -0.2391 \\
\texttt{runs/ode\_vae\_E65/ld5\_b0.02\_ls0.0002/run\_metadata.json} & \texttt{v5}, \(d=5\) & 0.0353 & -0.1382 \\
\texttt{runs/ode\_vae\_E65/ld8\_b0.02\_ls0.0005/run\_metadata.json} & \texttt{v5}, \(d=8\) & 0.0569 & -0.2022 \\
\texttt{src/pt\_files/final\_metrics.pt} & \texttt{v6}, no PCA & -- & 0.0956 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Manifold interpretability}
The codebase saves latent manifold projections (MDS) and reconstruction diagnostics for each run. Figure~\ref{fig:manifold} shows an example latent trajectory embedding from the trained model artifacts.

\begin{figure}[t]
\centering
\includegraphics[width=0.62\linewidth]{neuroscience/src/pt_files/latent_manifold_mds.png}
\caption{Latent manifold embedding produced by the ODE-VAE analysis pipeline.}
\label{fig:manifold}
\end{figure}

\section{Discussion}
The model captures the intended inductive bias: low-dimensional continuous latent trajectories with explicit geometric regularization. On synthetic data, this bias is highly effective. On real E65 recordings, however, results are sensitive to implementation and evaluation choices.

Three factors emerge from the repository artifacts:
\begin{enumerate}
\item \textbf{Metric-space mismatch.} PCA-space training can look favorable while strict raw-space \(R^2\) may degrade.
\item \textbf{Data-efficiency tradeoff.} Landmark subsampling (100 selected sequences from 180 usable trials) accelerates training but may reduce generalization.
\item \textbf{Optimization stability.} Strong regularization with small validation sets (3 trials) and stiff latent dynamics can produce unstable or negative final \(R^2\), despite early high points.
\end{enumerate}

These observations suggest that future gains likely require protocol-level changes in addition to architectural changes: larger and randomized holdout splits, early stopping on a stable cross-validated objective, trial-level (not frame-level) landmark selection, and direct raw-space reconstruction losses.

\section{Limitations and Reproducibility}
This study is bounded by available run artifacts in the repository and inherits version-specific logging differences. In particular, some run files report ``best'' and ``final'' \(R^2\) under different conditions, and not all checkpoints include identical metadata fields. We therefore report values exactly as saved in each artifact path. The codebase also indicates unresolved training fragility (including occasional NaN collapse), which should be addressed before definitive biological claims.

\section{Conclusion}
We presented a mathematically grounded latent Neural ODE-VAE framework for neural manifold modeling and analyzed the repository's \texttt{v1--v6} trajectory with \texttt{v5} as the primary model. The method can recover smooth low-dimensional dynamics and high synthetic reconstruction quality, but real-data performance remains sensitive to preprocessing and evaluation protocol. This work provides a formalized foundation and a clear set of engineering directions for turning ODE-VAE manifold modeling into a robust neuroscience analysis tool.

\paragraph{Societal impact.}
This work is basic research on neural representation learning from animal neuroscience data and has no immediate direct societal deployment. Potential long-term impact is improved scientific understanding of memory and cognition.

\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Aronov et~al.(2017)Aronov, Nevers, and Tank]{aronov2017}
Dmitriy Aronov, Rachel Nevers, and David W Tank.
\newblock Mapping of a non-spatial dimension by the hippocampal-entorhinal circuit.
\newblock \emph{Nature}, 543:719--722, 2017.

\bibitem[Bellmund et~al.(2018)Bellmund, Gardenfors, Moser, and Doeller]{bellmund2018}
Jacob L~S Bellmund, Peter Gardenfors, Edvard~I Moser, and Christian~F Doeller.
\newblock Navigating cognition: Spatial codes for human thinking.
\newblock \emph{Science}, 362:eaat6766, 2018.

\bibitem[Chaudhuri et~al.(2019)Chaudhuri, Gercek, Pandey, Peyrache, and Fiete]{chaudhuri2019}
Rishidev Chaudhuri, Burak Gercek, Bikash Pandey, Adrien Peyrache, and Ila Fiete.
\newblock The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep.
\newblock \emph{Nature Neuroscience}, 22:1512--1520, 2019.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{chen2018}
Ricky~TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud.
\newblock Neural ordinary differential equations.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Constantinescu et~al.(2016)Constantinescu, O'Reilly, and Behrens]{constantinescu2016}
Alexandra~O Constantinescu, Jill~X O'Reilly, and Timothy~EJ Behrens.
\newblock Organizing conceptual knowledge in humans with a grid-like code.
\newblock \emph{Science}, 352:1464--1468, 2016.

\bibitem[Eichenbaum and Cohen(2014)]{eichenbaum2014}
Howard Eichenbaum and Neal~J Cohen.
\newblock Can we reconcile the declarative memory and spatial navigation views on hippocampal function?
\newblock \emph{Neuron}, 83:764--770, 2014.

\bibitem[Gallego et~al.(2017)Gallego, Perich, Miller, and Solla]{gallego2017}
Juan~A Gallego, Matthew~G Perich, Lee~E Miller, and Sara~A Solla.
\newblock Neural manifolds for the control of movement.
\newblock \emph{Neuron}, 94:978--984, 2017.

\bibitem[Kingma and Welling(2014)]{kingma2014}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{ICLR}, 2014.

\bibitem[Low et~al.(2018)Low, Lewallen, Aronov, Nevers, and Tank]{low2018}
Ryan~J Low, Sean Lewallen, Dmitriy Aronov, Rachel Nevers, and David~W Tank.
\newblock Probing variability in a cognitive map using manifold inference from neural dynamics.
\newblock \emph{bioRxiv}, 2018.

\bibitem[MacDonald et~al.(2011)MacDonald, Lepage, Eden, and Eichenbaum]{macdonald2011}
Christopher~J MacDonald, Kyle~Q Lepage, Uri~T Eden, and Howard Eichenbaum.
\newblock Hippocampal time cells bridge the gap in memory for discontiguous events.
\newblock \emph{Neuron}, 71:737--749, 2011.

\bibitem[Nieh et~al.(2021)Nieh et~al.]{nieh2021}
Edward~H Nieh et~al.
\newblock Geometry of abstract learned knowledge in the hippocampus.
\newblock \emph{Nature}, 595:80--84, 2021.

\bibitem[O'Keefe and Dostrovsky(1971)]{okeefe1971}
John O'Keefe and Jonathan Dostrovsky.
\newblock The hippocampus as a spatial map.
\newblock \emph{Brain Research}, 34:171--175, 1971.

\bibitem[O'Keefe and Nadel(1978)]{okeefe1978}
John O'Keefe and Lynn Nadel.
\newblock \emph{The Hippocampus as a Cognitive Map}.
\newblock Clarendon Press, 1978.

\bibitem[Park et~al.(2020)Park, Miller, Nili, Ranganath, and Boorman]{park2020}
Sang~Ah Park, David~S Miller, Hamed Nili, Charan Ranganath, and Erie~D Boorman.
\newblock Map making: Constructing, combining, and inferring on abstract cognitive maps.
\newblock \emph{Neuron}, 107:1226--1238.e8, 2020.

\bibitem[Pnevmatikakis et~al.(2016)Pnevmatikakis et~al.]{pnevmatikakis2016}
Eftychios~A Pnevmatikakis et~al.
\newblock Simultaneous denoising, deconvolution, and demixing of calcium imaging data.
\newblock \emph{Neuron}, 89:285--299, 2016.

\bibitem[Pnevmatikakis and Giovannucci(2017)]{normcorre2017}
Eftychios~A Pnevmatikakis and Andrea Giovannucci.
\newblock NoRMCorre: An online algorithm for piecewise rigid motion correction of calcium imaging data.
\newblock \emph{Journal of Neuroscience Methods}, 291:83--94, 2017.

\bibitem[Recanatesi et~al.(2021)Recanatesi et~al.]{recanatesi2021}
Stefano Recanatesi et~al.
\newblock Predictive learning as a network mechanism for extracting low-dimensional latent space representations.
\newblock \emph{Nature Communications}, 12:1417, 2021.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019}
Yulia Rubanova, Ricky~TQ Chen, and David Duvenaud.
\newblock Latent ordinary differential equations for irregularly-sampled time series.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Russo et~al.(2018)Russo et~al.]{russo2018}
Abigail~A Russo et~al.
\newblock Motor cortex embeds muscle-like commands in an untangled population response.
\newblock \emph{Neuron}, 97:953--966.e8, 2018.

\bibitem[Schuck and Niv(2019)]{schuck2019}
Nicolas~W Schuck and Yael Niv.
\newblock Sequential replay of nonspatial task states in the human hippocampus.
\newblock \emph{Science}, 364:eaaw5181, 2019.

\bibitem[Stachenfeld et~al.(2017)Stachenfeld, Botvinick, and Gershman]{stachenfeld2017}
Kimberly~L Stachenfeld, Matthew~M Botvinick, and Samuel~J Gershman.
\newblock The hippocampus as a predictive map.
\newblock \emph{Nature Neuroscience}, 20:1643--1653, 2017.

\bibitem[Tenenbaum et~al.(2000)Tenenbaum, de Silva, and Langford]{tenenbaum2000}
Joshua~B Tenenbaum, Vin de Silva, and John~C Langford.
\newblock A global geometric framework for nonlinear dimensionality reduction.
\newblock \emph{Science}, 290:2319--2323, 2000.

\bibitem[Tolman(1948)]{tolman1948}
Edward~C Tolman.
\newblock Cognitive maps in rats and men.
\newblock \emph{Psychological Review}, 55:189--208, 1948.

\bibitem[Yu et~al.(2009)Yu et~al.]{yu2009}
Byron~M Yu et~al.
\newblock Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity.
\newblock \emph{Journal of Neurophysiology}, 102:614--635, 2009.

\end{thebibliography}

\end{document}
