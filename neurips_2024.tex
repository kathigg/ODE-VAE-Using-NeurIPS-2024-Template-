\documentclass{article}

% Omit the option for anonymized submission (with line numbers),
% and use [final] for the camera-ready version (no line numbers).
\usepackage[final]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[hidelinks]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{multirow}

\title{A Latent Neural ODE-VAE for Modeling Hippocampal Population Activity on Low-Dimensional Manifolds}

\author{%
  Kathleen Higgins \\
  University of Delaware \\
  \texttt{kathigg@udel.edu} \\
  \And
  Manuel Schottdorf \\
  University of Delaware \\
  \texttt{maschott@udel.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Neural population activity traces trajectories in a high-dimensional state space, yet accumulating evidence suggests these trajectories are confined to low-dimensional manifolds that encode both task variables and internal state. Existing manifold inference pipelines can recover geometry and explain variability, but often rely on multi-stage local models and do not impose globally smooth continuous-time dynamics. We develop a latent Neural ODE variational autoencoder (ODE-VAE) that jointly learns (i) a low-dimensional stochastic initial condition, (ii) continuous-time latent dynamics parameterized by a mixture-of-experts ODE, and (iii) a decoder back to neural activity. To better align reconstruction with temporally structured variability, we augment the variational objective with transition-consistency regularization in observation space and an LLE-inspired neighborhood reconstruction constraint in latent space. To reduce evaluation optimism on biological recordings, we split trials before fitting any normalization (and optional PCA) transforms, ensuring train-only preprocessing. On synthetic random-foraging sequences, the model achieves high reconstruction accuracy (\(R^2=0.9789\)). On the E65 hippocampal calcium dataset, a recent raw-space run (no PCA; latent dimension \(D=7\)) attains Pearson correlation \(r=0.7794\) and reconstruction \(R^2=0.6049\). Together, these results highlight both the promise and current fragility of end-to-end continuous-time manifold models for noisy biological recordings.
\end{abstract}

\section{Introduction}
Neural activity can be described as a point in a high-dimensional coordinate system, where each coordinate axis represents a single neuron's activity \citep{cunningham2014}. Underlying properties of the network and its inputs can confine neural trajectories to a subregion of this space, often referred to as a neural manifold \citep{cunningham2014,gallego2017}. The neural manifold has been proposed to underlie motor movements \citep{gallego2017,russo2018}, head direction cells \citep{chaudhuri2019}, and hippocampal maps of physical variables \citep{okeefe1971,frank2000,wood2000,okeefe1978}. The conceptual ideas in these studies suggest a general principle of hippocampal computation: the construction of organized maps of learned knowledge instantiated by neural manifolds \citep{tolman1948,okeefe1978,stachenfeld2017,bellmund2018,nieh2021}.

Nonlinear dimensionality reduction has demonstrated that neural population activity can often be described by 4--6 latent variables, suggesting that activity is constrained to a low-dimensional neural manifold that displays a geometric representation of both physical and abstract variables \citep{low2018,chaudhuri2019,nieh2021}. Existing approaches are limited to multi-stage machine-learning pipelines, using forest-based transition models (with probabilistic principal component analysis in decision-tree leaves) to define distances between population states, which are then embedded into a low-dimensional manifold and mapped back to neural activity for reconstruction \citep{low2018,tipping1999,breiman2001,tenenbaum2000,yu2009}. This piecewise approach partitions state space and models dynamics locally, hence lacking explicit enforcement of globally smooth latent dynamics and can exhibit saturation of reconstruction decoding performance with low-dimensional embeddings \citep{low2018}. Thus, we hypothesize that generative deep learning models offer a complementary framework: neural network architectures can be trained directly on biological neural population recordings to jointly learn low-dimensional latent coordinates, their temporal evolution, and the mapping back to neural activity \citep{kingma2014,chen2018,rubanova2019}.

In this paper, we propose a novel approach to modeling the neural manifold by constructing a Neural Ordinary Differential Equation variational autoencoder (ODE-VAE): a deep generative model that (i) encodes high-dimensional population activity into a low-dimensional latent state, (ii) models the evolution of that latent state as a continuous-time dynamical system parameterized by a neural ODE, and (iii) decodes the resulting latent trajectory back into neural activity \citep{kingma2014,chen2018,rubanova2019}. By training the encoder, dynamics, and decoder end-to-end under a variational objective, this approach aims to capture nonlinear manifold structure while imposing smooth temporal dynamics.
Our implementation uses mixture-of-experts latent dynamics and adds two regularizers inspired by manifold inference---transition-consistency in observation space and an LLE-inspired neighborhood reconstruction constraint in latent space \citep{low2018,saul2003}. We evaluate this family on synthetic and hippocampal calcium datasets and analyze the sensitivity of performance to preprocessing and evaluation choices.

\paragraph{Contributions.}
\begin{itemize}
\item We formalize an ODE-VAE for trialized population sequences with mixture-of-experts latent dynamics and explicit geometric regularizers.
\item We implement this formulation in a versioned codebase (\texttt{v1--v5}) and highlight a current configuration with train-only preprocessing, transition consistency, and an LLE-inspired neighborhood reconstruction regularizer.
\item We provide a reproducible evaluation on synthetic and hippocampal calcium datasets and identify protocol factors that strongly affect reconstruction metrics.
\end{itemize}

\section{Related Work}
Our approach lies at the intersection of manifold-based neuroscience and latent dynamical systems. In hippocampus, the cognitive map framework and subsequent experimental work motivate geometric organization of population codes \citep{tolman1948,okeefe1971,okeefe1978,eichenbaum2014}, including abstract and non-spatial representations \citep{aronov2017,tavares2015,constantinescu2016,schuck2019,park2020,nieh2021}. Beyond classical place coding, hippocampal population activity reflects trajectory and sequential organization \citep{frank2000,pastalkova2008,macdonald2011,taxidis2020}, episodic variables at shared locations \citep{wood2000,gill2011,mckenzie2014}, and multimodal/task variables such as odor and taste \citep{eichenbaum1987,herzog2019}. Manifold inference methods can recover low-dimensional structure and explain structured variability beyond measured task variables \citep{low2018,chaudhuri2019,rubin2019}.

In machine learning, variational autoencoders \citep{kingma2014} and neural ODEs \citep{chen2018} provide a principled framework for continuous-time latent-variable modeling. Latent ODEs extend this idea to irregularly sampled sequences \citep{rubanova2019}. We adopt this framework but tailor the encoder, evaluation protocol, and regularization to the neuroscience setting, emphasizing trialized sequences, explicit geometric constraints, and comparisons to MIND-style evaluation pipelines \citep{low2018}. For calcium imaging recordings, related methodological work has emphasized motion correction and demixing/denoising \citep{pnevmatikakis2016,normcorre2017}, highlighting the importance of preprocessing choices when evaluating reconstruction metrics.

\section{Problem Setup and Data}
We study trialized population activity sequences. Let \(y_b(t_\ell)\in\mathbb{R}^{N}\) denote the raw activity of \(N\) simultaneously recorded units/ROIs on trial \(b\in\{1,\dots,B\}\) at resampled time \(t_\ell\), where \(\ell\in\{1,\dots,L\}\) indexes a fixed-length grid. We write \(Y_b\in\mathbb{R}^{L\times N}\) for the stacked sequence.

\paragraph{Observation space.}
The implementation supports training either in raw ROI space or in a PCA-compressed space. Let \(x_b(t_\ell)\in\mathbb{R}^{P}\) denote the activity vector used by the model after preprocessing, where \(P=N\) for raw space and \(P=K\) when PCA is enabled. To avoid leakage, all normalization (and optional PCA) transforms are fit on training data only and then applied to validation/test sequences. Unless otherwise stated, reported metrics (\(r\), \(R^2\)) are computed in the evaluation space associated with each run.

\paragraph{Time grid.}
Trials are resampled to a common duration and the time vector is normalized to \([0,1]\); we denote the resulting grid by \(0=t_1<\cdots<t_L=1\). The latent dimension is denoted by \(D\).

\paragraph{E65 dataset.}
We use the Schottdorf Lab E65 dataset (\texttt{E65\_data.npz}), containing calcium activity (\(\Delta F/F\)) from \(N=375\) ROIs over \(T=7434\) frames, along with trial IDs and timestamps. Frames are grouped by trial, the first 10 trials are dropped, and each trial is linearly interpolated to a fixed length \(L=120\) (\texttt{trial\_len\_s=12}, \texttt{fps=10}), with time normalized to \([0,1]\). After filtering, 180 trials are available.

Our default ``no leakage'' preprocessing is trial-split-first: (i) construct trial sequences \(Y_b\); (ii) optionally subsample to 100 sequences via greedy landmark selection for speed; (iii) split trials into train/validation (default: hold out the last 3 sequences); (iv) fit preprocessing transforms on training only (per-feature normalization across all training frames and optional PCA), and apply the same transforms to validation. During training, an optional per-trial baseline is removed by subtracting the mean of the first 5 resampled bins.
Concretely, for training trials we compute per-feature moments
\begin{equation}
\mu = \frac{1}{B_{\mathrm{tr}}L}\sum_{b\in\mathcal{T}_{\mathrm{tr}}}\sum_{\ell=1}^{L} y_b(t_\ell),\qquad
\sigma = \sqrt{\frac{1}{B_{\mathrm{tr}}L}\sum_{b\in\mathcal{T}_{\mathrm{tr}}}\sum_{\ell=1}^{L}\big(y_b(t_\ell)-\mu\big)^{\odot 2}}+\varepsilon,
\end{equation}
and normalize all trials as \(x_b(t_\ell)=(y_b(t_\ell)-\mu)\oslash\sigma\), where \(\oslash\) and \(\odot\) denote elementwise division and multiplication and \(\varepsilon=10^{-8}\). If baseline correction is enabled, we further set
\(\tilde{x}_b(t_\ell)=x_b(t_\ell)-\frac{1}{5}\sum_{j=1}^{5}x_b(t_j)\).

\paragraph{Synthetic benchmark.}
We additionally evaluate on \texttt{synthetic\_rat\_data.npz} (4000 frames, 300 neurons, 20 trials), which provides a controlled benchmark for recoverability of smooth low-dimensional dynamics.

\section{Model: Latent Neural ODE-VAE}
\subsection{Stochastic encoder}
For each trial, an encoder network parameterizes a diagonal Gaussian posterior on the latent initial state. Let \(X_b=[x_b(t_1),\dots,x_b(t_L)]\in\mathbb{R}^{L\times P}\) denote the preprocessed trial sequence in the model's observation space. The approximate posterior is
\begin{equation}
q_\phi(z_{0,b} \mid X_b) = \mathcal{N}\!\left(\mu_b,\operatorname{diag}(\sigma_b^2)\right),
\end{equation}
with reparameterization
\begin{equation}
z_{0,b} = \mu_b + \sigma_b \odot \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I).
\end{equation}
Here \(z_{0,b}\in\mathbb{R}^D\), \(\mu_b\in\mathbb{R}^D\), \(\sigma_b\in\mathbb{R}^D_{>0}\), and \(\odot\) denotes elementwise multiplication.
In our current configuration, \((\mu_b,\log\sigma_b^2)\) are produced by a Transformer sequence encoder applied to \(X_b\), but MLP and recurrent encoders are also supported.

\subsection{Continuous-time latent dynamics}
Latent trajectories are generated by a neural ODE:
\begin{equation}
\frac{dz_b(t)}{dt} = f_\theta(z_b(t)), \qquad z_b(t_1)=z_{0,b}.
\end{equation}
We parameterize \(f_\theta\) as a mixture of experts:
\begin{equation}
f_\theta(z) = \sum_{e=1}^{E} \pi_e(z) f_e(z), \qquad
\pi(z)=\operatorname{softmax}(g(z)),
\end{equation}
In the reported experiments, this defines an autonomous ODE (no explicit dependence on \(t\)); time dependence can be introduced by conditioning the gate and experts on a learned embedding of \(t\).
We use \(E=4\) latent experts by default.
Each expert \(f_e:\mathbb{R}^D\to\mathbb{R}^D\) is an MLP and \(\pi_e(z)\in[0,1]\) are gating weights satisfying \(\sum_e \pi_e(z)=1\).

\subsection{Decoder family}
A decoder maps latent states back to observations:
\begin{equation}
\hat{x}_b(t_\ell)=g_\psi(z_b(t_\ell)).
\end{equation}
The codebase supports MLP, neuron-aware, local-attention, and mixture-of-experts (MoE) decoders; our current configuration uses an MoE decoder with 8 decoder experts. In all cases, \(g_\psi:\mathbb{R}^D\to\mathbb{R}^{P}\) outputs the mean of a factorized Gaussian observation model in the chosen observation space.

\section{Training Objective and Regularization}
We optimize a variational objective with auxiliary regularizers. Under a Gaussian observation model \(p_\psi(x_b(t_\ell)\mid z_b(t_\ell))=\mathcal{N}(g_\psi(z_b(t_\ell)),\sigma^2 I)\) with fixed \(\sigma^2\), maximizing the standard ELBO (\(\beta=1\)) corresponds (up to constants and a scale factor) to minimizing mean-squared reconstruction error plus a KL penalty. In our experiments we use a weighted-KL variant (\(\beta\)-VAE), with \(\beta_t\) warmed up to a final \(\beta\).

The base objective combines reconstruction and KL terms:
\begin{equation}
\mathcal{L}_{\text{base}} = \mathcal{L}_{\text{rec}} + \beta\,\mathcal{L}_{\text{KL}},
\end{equation}
where
\begin{equation}
\mathcal{L}_{\text{rec}} = \frac{1}{B\,L\,P}\sum_{b=1}^{B}\sum_{\ell=1}^{L}\|\hat{x}_b(t_\ell)-x_b(t_\ell)\|_2^2,
\end{equation}
\begin{equation}
\mathcal{L}_{\text{KL}} = \frac{1}{B}\sum_b D_{\text{KL}}\big(q_\phi(z_{0,b}\mid X_b)\,\|\,\mathcal{N}(0,I)\big).
\end{equation}
Equivalently, the (negative) \(\beta\)-VAE objective per trial is
\begin{equation}
\mathcal{L}_{\beta\text{-VAE}} =
-\mathbb{E}_{q_\phi(z_{0,b}\mid X_b)}\Big[\sum_{\ell=1}^{L}\log p_\psi(x_b(t_\ell)\mid z_b(t_\ell))\Big]
+\beta\,D_{\mathrm{KL}}\big(q_\phi(z_{0,b}\mid X_b)\,\|\,p(z_{0,b})\big),
\end{equation}
which reduces to the negative ELBO when \(\beta=1\). We use prior \(p(z_{0,b})=\mathcal{N}(0,I)\). In practice, the code uses a single Monte Carlo sample of \(z_{0,b}\) per trial and minibatch.

\paragraph{Smoothness regularization.}
\begin{equation}
\mathcal{L}_{\text{smooth}} = \frac{1}{B(L-1)D}\sum_{b,\ell}
\left\|\frac{z_b(t_{\ell+1})-z_b(t_\ell)}{t_{\ell+1}-t_\ell}\right\|_2^2.
\end{equation}

\paragraph{Transition-aware regularization.}
\begin{equation}
\mathcal{L}_{\text{trans}} = \frac{1}{B(L-1)P}\sum_{b,\ell}
\left\|\big(\hat{x}_b(t_{\ell+1})-\hat{x}_b(t_\ell)\big)-\big(x_b(t_{\ell+1})-x_b(t_\ell)\big)\right\|_2^2.
\end{equation}
This term is linearly warmed up for the first 30 epochs.

\paragraph{LLE-inspired neighborhood reconstruction regularization.}
For flattened latent points \(\{z_i\}_{i=1}^{M_{\mathrm{LLE}}}\subset\mathbb{R}^D\), with \(k\)-NN set \(\mathcal{N}_k(i)\), we add a neighborhood reconstruction penalty inspired by locally linear embedding (LLE) \citep{saul2003}:
\begin{equation}
\mathcal{L}_{\text{LLE}} = \frac{1}{M_{\mathrm{LLE}}D}\sum_{i=1}^{M_{\mathrm{LLE}}}
\left\|z_i-\sum_{j\in\mathcal{N}_k(i)}w_{ij}z_j\right\|_2^2,
\quad
w_{ij}=\frac{\exp\!\left(-\frac{\|z_i-z_j\|_2}{\tau}\right)}{\sum_{j'\in\mathcal{N}_k(i)}\exp\!\left(-\frac{\|z_i-z_{j'}\|_2}{\tau}\right)}.
\end{equation}
Unlike classical LLE, we do not solve for per-point constrained least-squares weights; instead we use kernel-normalized weights, yielding a simple differentiable local-consistency regularizer.
Default parameters: \(k=8\), \(M_{\mathrm{LLE}}\le 256\), \(\tau=0.1\).

\paragraph{Total loss.}
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{rec}} + \beta_t\mathcal{L}_{\text{KL}} +
\lambda_{\text{smooth}}\mathcal{L}_{\text{smooth}} +
\lambda_{\text{trans},t}\mathcal{L}_{\text{trans}} +
\lambda_{\text{LLE}}\mathcal{L}_{\text{LLE}}.
\end{equation}
The KL coefficient \(\beta_t\) is warmed up over 30 epochs to a final value \(\beta=0.02\).

\section{Versioned Model Development and Failure Modes}
The repository contains a sequence of incrementally modified training scripts that reflect both model and pipeline iteration: the current implementation lives in \texttt{v5\_neural\_vae.py} and earlier variants are archived under \texttt{neuroscience/src/archived\_ode\_models/}. These versions should not be interpreted as a clean ablation study: they differ in architecture, preprocessing, evaluation space, solver choices, and logging conventions. Nevertheless, documenting this evolution is useful for understanding observed fragilities and for motivating the transition and neighborhood reconstruction regularizers, which are directly inspired by the geometry- and transition-aware components of the MIND pipeline \citep{low2018,saul2003}.

\paragraph{\texttt{v1}: baseline latent Neural ODE-VAE.}
\texttt{v1} implements the minimal continuous-time VAE setup \citep{kingma2014,chen2018}: an MLP encoder of \(x(t_1)\), a single latent vector field \(f_\theta\), an MLP decoder, and an \(\ell_2\) smoothness penalty on finite-difference latent velocities. It also introduces global PCA preprocessing and greedy landmark selection, mirroring common manifold inference practice \citep{low2018}.
\emph{Vulnerabilities:} (i) PCA is fit on the full recording before the train/val split, which can leak test-set structure into the representation; (ii) landmark selection is performed on flattened time points and mapped back to trials via a modulo operation, which can duplicate trials and bias the subsample away from true trial-level coverage; (iii) the default ordered holdout (holding out the last few trials) is sensitive to nonstationarities or ordering effects.
\emph{Motivation for \texttt{v2}:} reduce protocol-induced optimism by splitting first and fitting normalization/PCA only on training data, and explore more expressive dynamics.

\paragraph{\texttt{v2}: switching/gated latent dynamics with train-only preprocessing.}
\texttt{v2} adds a learned gating network over multiple candidate latent vector fields (a switching/mixture-style dynamics), increasing expressivity beyond a single global \(f_\theta\). Crucially, \texttt{v2} builds raw trial sequences first, then performs the train/val split, and fits both standardization and PCA on training data only; evaluation reconstructs back to raw ROI space via inverse PCA before scoring, which is closer to MIND-style reporting \citep{low2018}.
\emph{Vulnerabilities:} (i) time normalization to \([0,1]\) is disabled in the script, which changes the effective scale seen by the ODE solver and can make optimization more sensitive; (ii) gating dynamics introduce additional nonconvexity and can collapse to a single expert without careful tuning.
\emph{Motivation for \texttt{v3}:} incorporate mixture-of-experts dynamics and richer decoders to better capture heterogeneous neural tuning and trial-to-trial variability.

\paragraph{\texttt{v3}: mixture-of-experts latent ODE and decoder variants.}
\texttt{v3} introduces a mixture-of-experts latent vector field (soft gating over multiple \(f_e\)) and a family of decoders (neuron-aware, local-attention, and MoE decoders) intended to better model neuron-specific heterogeneity. It also adds an optional per-trial baseline correction (subtracting early-bin means) to reduce drift/offset burden on the latent state.
\emph{Vulnerabilities:} (i) the preprocessing/evaluation pipeline reverts to full-session PCA and PCA-space scoring, making results harder to compare to raw-space metrics and potentially optimistic; (ii) the flattened-time landmark subsampling and ordered-holdout issues from \texttt{v1} persist; (iii) added model capacity increases overfitting risk when validation is extremely small.
\emph{Motivation for \texttt{v4}:} refine the decoder locality bias and improve hardware compatibility (notably Apple MPS) while keeping the MoE latent dynamics.

\paragraph{\texttt{v4}: MoE latent dynamics with locality-biased decoders (MPS-safe).}
\texttt{v4} largely preserves \texttt{v3}'s MoE latent dynamics and decoder choices, and emphasizes MPS-safe ODE integration (fixed-step fallbacks) to reduce device-specific solver failures during experimentation.
\emph{Vulnerabilities:} decoder ``locality'' is primarily architectural (attention-like) rather than enforced by an explicit geometric objective, and the pipeline-level issues (PCA leakage, trial subsampling bias, small/ordered holdout) remain.
\emph{Motivation for \texttt{v5}:} add explicit constraints that directly regularize temporal transitions and local manifold geometry, closer in spirit to MIND's use of transition structure and neighborhood geometry \citep{low2018,saul2003}.

\paragraph{\texttt{v5}: transition consistency + neighborhood reconstruction regularization.}
The current model retains MoE latent dynamics and MoE decoding while adding two explicit regularizers: (i) a transition-consistency loss in observation space that matches \(\Delta \hat{x}(t)\) to \(\Delta x(t)\) and (ii) an LLE-inspired neighborhood reconstruction penalty that encourages each latent point to be reconstructible from its \(k\)-NN neighborhood \citep{saul2003}. Both are motivated by the observation that reconstruction alone can ignore fine-grained temporal and local geometric structure, which MIND leverages via transition-aware distances and local mappings \citep{low2018}.
\emph{Vulnerabilities:} (i) the default training script still subsamples training data using flattened-time landmark selection and uses a small ordered validation set, amplifying sensitivity to preprocessing and random seed; (ii) the codebase supports multiple evaluation spaces (PCA vs raw), so reported \(R^2\) values are not directly comparable unless the evaluation definition is matched.

\section{Experimental Protocol}
\subsection{Configurations}
Main settings (from \texttt{neuroscience/configs/v5\_base.txt} and saved run configs): latent dimension \(D=7\), batch size 8, 150 epochs, Adam optimizer (learning rate 0.001, weight decay \(10^{-5}\)), KL coefficient \(\beta=0.02\) with a 30-epoch warmup, \(\lambda_{\text{smooth}}=10^{-3}\), \(\lambda_{\text{trans}}=5\times10^{-3}\) with a 30-epoch warmup, \(\lambda_{\text{LLE}}=5\times10^{-3}\), landmark count 100, and baseline correction enabled. To avoid preprocessing leakage, we enable train-only preprocessing (trial split before fitting normalization and optional PCA transforms).

\paragraph{Implementation details.}
The current configuration uses a lightweight Transformer sequence encoder (hidden width 256, 1 layer, 2 heads) with pooling on the first token to parameterize \(q_\phi(z_0\mid X)\). The latent vector field uses \(E=4\) experts with hidden width 128 and a learned gating network; derivatives are layer-normalized for stability. For reconstruction, we use a mixture-of-experts decoder with 8 decoder experts and hidden width 256. Latent dynamics are integrated with Dormand--Prince (\texttt{dopri5}) using tolerances \texttt{rtol=$10^{-2}$} and \texttt{atol=$10^{-3}$}. Gradients are clipped to max norm 1.0.

\subsection{Metrics}
We report both Pearson correlation coefficient and coefficient of determination on validation sequences. For a validation set, we compute
\begin{equation}
r = \mathrm{corr}\!\left(\mathrm{vec}(X_{\mathrm{val}}), \mathrm{vec}(\hat{X}_{\mathrm{val}})\right),
\end{equation}
where \(\mathrm{vec}(\cdot)\) denotes vectorization over trial, time, and feature dimensions. We additionally compute coefficient of determination,
\begin{equation}
R^2 = 1-\frac{\sum_{b,\ell}\|x_b(t_\ell)-\hat{x}_b(t_\ell)\|_2^2}{\sum_{b,\ell}\|x_b(t_\ell)-\bar{x}\|_2^2},
\end{equation}
where \(\bar{x}=\frac{1}{B\,L}\sum_{b,\ell} x_b(t_\ell)\) denotes the mean activity vector across all validation entries in the evaluation space.
Metrics can be computed either in raw ROI space or in PCA space. When PCA is enabled, raw-space scoring is obtained by applying inverse PCA and de-normalization to \(\hat{x}\) before computing \(r\) and \(R^2\), which makes comparisons to MIND-style reconstruction metrics more direct \citep{low2018}.

\section{Results}
\subsection{E65 hippocampal data: effect of preprocessing and evaluation space}
Runs in this repository differ not only in model capacity but also in \emph{where} reconstruction is optimized and \emph{where} metrics are computed (raw ROI space vs.\ PCA space). Table~\ref{tab:e65} reports the latest E65 metrics for three closely matched ODE-VAE runs (all share the same hyperparameters and no-leakage preprocessing) that differ only by whether PCA is applied (\texttt{use\_pca}) and whether metrics are computed in raw vs.\ PCA space (\texttt{eval\_metrics\_pca}). Training in PCA space but scoring in raw ROI space (\texttt{2026-02-22\_155529\_e7f3cc6}) yields \(R^2=0.4010\), whereas training and scoring in raw ROI space (no PCA; \texttt{2026-02-22\_160802\_b03d3ed}) improves to \(R^2=0.6049\). Scoring in PCA space (\texttt{2026-02-22\_155909\_e7f3cc6}) produces higher values (\(R^2=0.7241\)), but these are not directly comparable to raw-space metrics because PCA compresses and reweights variance. For context, Table~\ref{tab:e65} also includes a MIND baseline \citep{low2018}; note that MIND uses a random 90/10 split while our default ODE-VAE evaluation holds out the last 3 trials.
Figure~\ref{fig:learning_curve} shows the corresponding learning curve for the raw-space run.

\begin{table}[t]
\centering
\caption{E65 reconstruction metrics under different training/evaluation spaces. ``Dim'' denotes embedding dimension \(d\) for MIND and latent dimension \(D\) for ODE-VAE. PCA-space metrics are not directly comparable to raw ROI metrics.}
\label{tab:e65}
\begin{tabular}{lccccc}
\toprule
Method & Train & Eval & Dim & \(r\) & \(R^2\) \\
\midrule
MIND \citep{low2018} & PCA & raw & 7 & 0.7237 & 0.5236 \\
ODE-VAE (PCA \(\rightarrow\) raw) & PCA & raw & 7 & 0.6658 & 0.4010 \\
ODE-VAE (raw \(\rightarrow\) raw) & raw & raw & 7 & 0.7794 & 0.6049 \\
ODE-VAE (PCA \(\rightarrow\) PCA) & PCA & PCA & 7 & 0.8627 & 0.7241 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{neuroscience/figures/learning_curve_2026-02-22_160802_b03d3ed.png}
\caption{Learning curve for the E65 raw \(\rightarrow\) raw run (\texttt{2026-02-22\_160802\_b03d3ed}).}
\label{fig:learning_curve}
\end{figure}

\subsection{Manifold interpretability}
The codebase saves latent manifold projections (MDS) and reconstruction diagnostics for each run. Figure~\ref{fig:manifold} shows an example latent trajectory embedding from the trained model artifacts.

\begin{figure}[t]
\centering
\includegraphics[width=0.62\linewidth]{neuroscience/figures/latent_manifold_mds_2026-02-22_160802_b03d3ed.png}
\caption{Latent manifold embedding (MDS) produced by the ODE-VAE analysis pipeline (\texttt{2026-02-22\_160802\_b03d3ed}).}
\label{fig:manifold}
\end{figure}

\section{Discussion}
The model captures the intended inductive bias: low-dimensional continuous latent trajectories with explicit geometric regularization. On synthetic data, this bias is highly effective. On real E65 recordings, however, results are sensitive to implementation and evaluation choices.

Three factors emerge from the saved run artifacts:
\begin{enumerate}
\item \textbf{Metric-space mismatch.} PCA-space training can look favorable while strict raw-space \(R^2\) may degrade. This is particularly salient when comparing to MIND-style evaluations, which reconstruct back to neuron space (via inverse PCA) before scoring \citep{low2018}.
\item \textbf{Data-efficiency tradeoff.} Landmark subsampling (100 selected sequences from 180 usable trials) accelerates training but may reduce generalization. In MIND, landmarks primarily support graph construction and embedding; the learned mapping is then applied to all eligible time points \citep{low2018}.
\item \textbf{Optimization stability.} Strong regularization with small validation sets (3 trials) and stiff latent dynamics can produce unstable or negative final \(R^2\), despite early high points.
\end{enumerate}

These observations suggest that future gains likely require protocol-level changes in addition to architectural changes: larger and randomized holdout splits, early stopping on a stable cross-validated objective, trial-level (not frame-level) landmark selection, and direct raw-space reconstruction losses.

\section{Future Work}
A central motivation of this project is to connect end-to-end continuous-time latent dynamical modeling with the multi-stage manifold inference pipeline used in MIND \citep{low2018}. Our current codebase already adopts several MIND-inspired components (global PCA preprocessing, greedy landmark selection for visualization, and MDS-based manifold plots), but the modeling philosophy differs: MIND estimates a graph of transition structure via a PPCA regression forest and learns explicit local mappings between ambient activity and manifold coordinates \citep{low2018,tipping1999,breiman2001}, whereas the ODE-VAE learns a single global generative model (encoder + latent dynamics + decoder) by optimizing a reconstruction objective \citep{kingma2014,chen2018,rubanova2019}.
Below we outline concrete directions to tighten this connection and improve robustness on calcium recordings.

\subsection{Match MIND-style evaluation protocols and metrics}
Many apparent discrepancies across saved E65 runs are consistent with evaluation-definition mismatch. In the MIND Matlab cross-validation script, trials are split randomly (e.g., 90/10), reconstruction is scored in the original neuron space after mapping back through inverse PCA, and performance is visualized both as an overall score and as per-trial dots \citep{low2018}. Aligning our training and reporting with this protocol would make comparisons substantially more interpretable.
Concretely, we plan to (i) report both Pearson correlation on vectorized activity blocks,
\begin{equation}
r = \mathrm{corr}\!\left(\mathrm{vec}(Y_{\mathrm{test}}),\mathrm{vec}(\hat{Y}_{\mathrm{test}})\right),
\end{equation}
and variance-explained \(R^2\) under repeated random trial splits, and (ii) include held-out neuron evaluation where latents are inferred from a subset of neurons and used to predict excluded neurons, mirroring the ``cell prediction'' analyses in MIND \citep{low2018}. This will also require revisiting the current practice of validating on the final 3 trials, which can conflate generalization with drift.

\subsection{Use landmarks for geometry, not for shrinking the training set}
In MIND, landmarks are an efficiency device for graph construction and embedding; the learned mapping is then applied to all eligible time points \citep{low2018}. In contrast, the default configuration further subsamples the dataset down to 100 landmarked sequences (from 180 trials), which likely increases estimator variance and can bias which trials are emphasized during training.
A straightforward next step is to train the ODE-VAE on all trials/time points and reserve landmark selection for: (i) visualization, (ii) neighbor graph construction for local regularizers, and (iii) lightweight geometric diagnostics (e.g., random-walk distance embeddings). This change should directly improve stability without changing the model class.

\subsection{Hybrid decoders: combine global reconstruction with MIND-like local mappings}
The MIND pipeline learns mappings between ambient PCA space and manifold coordinates using locally weighted methods (e.g., LLE regression) \citep{saul2003,low2018}. This provides a natural mechanism to capture sharp, local irregularities that global regressors may smooth out. Our current decoders are global function approximators (MLP/MoE), which can yield good coarse reconstructions but may miss neuron-specific transients.
An appealing hybrid is a global decoder plus a local residual term defined over nearby latent states,
\begin{equation}
\hat{x}(t) = g_\psi(z(t)) + \sum_{j\in\mathcal{N}_k(z(t))} \alpha_j(z(t))\,r_j,
\end{equation}
where \(\mathcal{N}_k(\cdot)\) are neighbors in latent space (or in a MIND-style random-walk metric), \(r_j\in\mathbb{R}^K\) are learned prototype residuals, and \(\alpha_j\) are normalized weights (e.g., softmax over distances). This would preserve the interpretability and global smoothness of the ODE while injecting the kind of local adaptivity that MIND's mapping stage provides.

\subsection{Optimize and score in raw neuron space (with PCA as an internal linear layer)}
Several E65 runs in this repository train and score in different spaces (PCA vs raw ROI), making \(R^2\) values hard to compare. MIND keeps PCA primarily as a compression step but reconstructs back to the original activity space before computing reconstruction scores \citep{low2018}. A direct analogue for the ODE-VAE is to keep a fixed (or lightly fine-tuned) PCA projection for computational efficiency, but decode back to raw ROI space and compute the main reconstruction loss on \(y_b(t)\in\mathbb{R}^N\).
One implementation is to parameterize a raw-space decoder as
\(\hat{y}(t) = W_{\mathrm{PCA}}^\top \hat{x}(t) + \mu\),
using the PCA loading matrix \(W_{\mathrm{PCA}}\) and mean \(\mu\) from preprocessing, and to define \(\mathcal{L}_{\mathrm{rec}}\) in raw space. This would more closely match the scientific question---reconstructing neural activity---and reduce the chance that good PCA-space fits hide biologically relevant errors.

\subsection{Make latent dynamics probabilistic to better match MIND transition structure}
MIND estimates transition structure via a probabilistic model of next-step activity (a PPCA regression forest) and then derives a random-walk geometry from transition probabilities \citep{low2018,tipping1999,breiman2001}. Our latent ODE is deterministic given \(z_{0,b}\), which can be brittle when real data exhibit unmodeled inputs, nonstationarities, or observation noise.
A natural extension is to introduce process noise (Neural SDEs) or discrete-time stochastic residuals,
\(z(t_{\ell+1}) = z(t_\ell) + \int_{t_\ell}^{t_{\ell+1}} f_\theta(z(t))\,dt + \eta_\ell\),
which can absorb variability not explained by the initial condition while retaining smooth latent structure. This direction also creates a clearer conceptual bridge between ODE-based dynamics and MIND's transition-probability graph.

\subsection{Geometry-aware objectives beyond neighborhood reconstruction}
Our current neighborhood reconstruction penalty encourages local consistency in the learned latent point cloud, but it does not directly use transition structure. The MIND code constructs local distances from transition probabilities (e.g., \(d_{ij}\propto \sqrt{-\log p_{ij}}\)) and then computes geodesic distances on the resulting graph before embedding \citep{low2018}. This is conceptually related to geodesic-distance embeddings in nonlinear dimensionality reduction \citep{tenenbaum2000}. A promising direction is to import this idea as a regularizer: estimate a transition graph among landmarked latent points, compute a random-walk geodesic distance matrix, and penalize distortions between these distances and Euclidean distances in the latent embedding. Such a constraint could encourage the latent representation to respect the sequential structure that MIND leverages, while still permitting an end-to-end generative model.

\section{Limitations and Reproducibility}
This study is bounded by the available run artifacts and inherits version-specific logging differences. In particular, some run files report ``best'' and ``final'' \(R^2\) under different conditions, and not all checkpoints include identical metadata fields. We therefore report values exactly as saved in each artifact path. Moving forward, two priorities are (i) systematic sweeps over latent dimension and (ii) matching evaluation protocols (trial splits, preprocessing, and metric definitions) to enable one-to-one comparisons across runs and methods. The implementation also exhibits training fragility (including occasional NaN divergence), which should be addressed before drawing definitive biological conclusions.

\section{Conclusion}
We presented a mathematically grounded latent Neural ODE-VAE framework for neural manifold modeling and analyzed a sequence of model variants (\texttt{v1--v5}). The method can recover smooth low-dimensional dynamics and high synthetic reconstruction quality, but real-data performance remains sensitive to preprocessing and evaluation protocol. This work provides a formal foundation and concrete directions for improving robustness of ODE-VAE manifold modeling for neuroscience.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
