\documentclass{article}

\usepackage{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{multirow}

\title{A Latent Neural ODE-VAE for Modeling Hippocampal Population Activity on Low-Dimensional Manifolds}

\author{%
  Anonymous Author(s) \\
  Affiliation \\
  Address \\
  \texttt{anonymous@neurips.cc} \\
}

\begin{document}

\maketitle

\begin{abstract}
Neural population activity traces trajectories in a high-dimensional state space, yet accumulating evidence suggests these trajectories are confined to low-dimensional manifolds that encode both task variables and internal state. Existing manifold inference pipelines can recover geometry and explain variability, but often rely on multi-stage local models and do not impose globally smooth continuous-time dynamics. We develop a latent Neural ODE variational autoencoder (ODE-VAE) that jointly learns (i) a low-dimensional stochastic initial condition, (ii) continuous-time latent dynamics parameterized by a mixture-of-experts ODE, and (iii) a decoder back to neural activity. To better align reconstruction with temporally structured variability, our \texttt{v5} implementation adds transition-consistency regularization in observation space and a soft locally linear embedding (LLE) constraint in latent space. On synthetic random-foraging sequences, the model achieves high reconstruction accuracy (\(R^2=0.9789\)) while exhibiting seed sensitivity. On the E65 hippocampal calcium dataset, we observe moderate reconstruction in archived runs (\(R^2=0.4368\)) and substantial dependence on preprocessing, split strategy, and evaluation space. Together, these results highlight both the promise and current fragility of end-to-end continuous-time manifold models for noisy biological recordings.
\end{abstract}

\section{Introduction}
Neural activity can be described as a point in a high-dimensional space, where each coordinate axis corresponds to a neuron's activity. Despite this ambient dimensionality, population trajectories frequently occupy structured, low-dimensional manifolds. Such manifolds have been implicated in motor control \citep{gallego2017,russo2018}, spatial coding in hippocampus \citep{okeefe1971,okeefe1978}, and the representation of non-spatial variables \citep{aronov2017,nieh2021}. These findings motivate a geometric view of hippocampal computation: learned knowledge may be organized as trajectories on manifolds that encode both physical and abstract structure \citep{tolman1948,stachenfeld2017,bellmund2018}.

Recent hippocampal manifold analyses estimate intrinsic dimensionalities on the order of 4--6 and show that manifold geometry can capture both task variables and structured trial-to-trial variability \citep{low2018,chaudhuri2019,nieh2021}. However, many approaches are multi-stage: they model transitions locally, define a data-dependent distance, embed points into a low-dimensional coordinate system, and then learn a separate reconstruction mapping \citep{low2018,tenenbaum2000,yu2009}. While effective for geometry, these pipelines do not directly couple representation learning, temporal evolution, and reconstruction in a single continuous-time generative model.

We propose a latent Neural ODE variational autoencoder (ODE-VAE) for trialized population recordings. The model encodes trial onset activity into a stochastic latent initial condition, evolves it via a continuous-time latent ODE, and decodes latent trajectories back to neural activity. Our \texttt{v5} implementation introduces mixture-of-experts latent dynamics and adds two regularizers inspired by manifold inference: (i) transition consistency in observation space to encourage accurate step-to-step changes, and (ii) a soft LLE constraint to encourage locally linear latent structure without forcing a globally linear embedding. We evaluate this family on synthetic and hippocampal calcium datasets and analyze the sensitivity of performance to preprocessing and evaluation choices.

\paragraph{Contributions.}
\begin{itemize}
\item We formalize an ODE-VAE for trialized population sequences with mixture-of-experts latent dynamics and explicit geometric regularizers.
\item We instantiate this formulation in a versioned codebase (\texttt{v1--v6}) and present \texttt{v5} as the primary model with transition and soft-LLE regularization.
\item We provide a reproducible evaluation on synthetic and hippocampal calcium datasets and identify protocol factors that strongly affect reconstruction metrics.
\end{itemize}

\section{Related Work}
Our approach lies at the intersection of manifold-based neuroscience and latent dynamical systems. In hippocampus, the cognitive map framework and subsequent experimental work motivate geometric organization of population codes \citep{okeefe1971,okeefe1978,eichenbaum2014}, including abstract and non-spatial representations \citep{constantinescu2016,schuck2019,park2020,aronov2017,nieh2021}. Manifold inference from neural dynamics can recover low-dimensional structure and explain structured variability beyond measured task variables \citep{low2018,chaudhuri2019}.

In machine learning, variational autoencoders \citep{kingma2014} and neural ODEs \citep{chen2018} provide a principled framework for continuous-time latent-variable modeling. Latent ODEs extend this idea to irregularly sampled sequences \citep{rubanova2019}. We adopt this framework but tailor the encoder, evaluation protocol, and regularization to the neuroscience setting, emphasizing trialized sequences, explicit geometric constraints, and manifold visualizations that facilitate comparison to prior manifold studies.

\section{Problem Setup and Data}
We study trialized population activity sequences. Let \(y_b(t_\ell)\in\mathbb{R}^{N}\) denote the raw activity of \(N\) simultaneously recorded units/ROIs on trial \(b\in\{1,\dots,B\}\) at resampled time \(t_\ell\), where \(\ell\in\{1,\dots,L\}\) indexes a fixed-length grid. We write \(Y_b\in\mathbb{R}^{L\times N}\) for the stacked sequence.

\paragraph{Observation space.}
In \texttt{v5}, the model is trained on a PCA-projected representation of activity. Let \(x_b(t_\ell)\in\mathbb{R}^{K}\) be the \(K\)-dimensional PCA coordinate at time \(t_\ell\), and let \(X_b\in\mathbb{R}^{L\times K}\) be the corresponding trial sequence. Unless otherwise stated, all losses and reported \(R^2\) values for \texttt{v5} are computed in this PCA space. For some evaluations (e.g., the optional sweep path), reconstructions are mapped back to raw ROI space via inverse PCA and de-normalization.

\paragraph{Time grid.}
Trials are resampled to a common duration and the time vector is normalized to \([0,1]\); we denote the resulting grid by \(0=t_1<\cdots<t_L=1\). The latent dimension is denoted by \(D\).

\paragraph{E65 dataset.}
We use the Schottdorf Lab E65 dataset (\texttt{E65\_data.npz}), containing calcium activity (\(\Delta F/F\)) from \(N=375\) ROIs over \(T=7434\) frames, along with trial IDs, timestamps, and aligned behavioral covariates. In the \texttt{v5} preprocessing path: (i) PCA is fit to the full recording and retains 95\% variance, producing \(K=129\) components; (ii) frames are grouped by trial, the first 10 trials are dropped, and each trial is linearly interpolated to a fixed length \(L=120\) (\texttt{trial\_len\_s=12}, \texttt{fps=10}); (iii) the time vector is normalized to \([0,1]\); (iv) each PCA component is standardized over time (session-level z-score) and an optional per-trial baseline is removed by subtracting the mean of the first 5 resampled bins. After filtering, 180 trials are available; default validation holds out the last 3 trials (train 177 / val 3). For efficiency, the default configuration further subsamples to 100 sequences via greedy landmark selection.

\paragraph{Synthetic benchmark.}
We additionally evaluate on \texttt{synthetic\_rat\_data.npz} (4000 frames, 300 neurons, 20 trials), which provides a controlled benchmark for recoverability of smooth low-dimensional dynamics.

\section{Model: Latent Neural ODE-VAE}
\subsection{Stochastic encoder}
For each trial, the encoder uses only \(x_b(t_1)\) and outputs a diagonal Gaussian posterior on the latent initial state:
\begin{equation}
q_\phi(z_{0,b} \mid x_b(t_1)) = \mathcal{N}\!\left(\mu_b,\operatorname{diag}(\sigma_b^2)\right),
\end{equation}
with reparameterization
\begin{equation}
z_{0,b} = \mu_b + \sigma_b \odot \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I).
\end{equation}
Here \(z_{0,b}\in\mathbb{R}^D\), \(\mu_b\in\mathbb{R}^D\), \(\sigma_b\in\mathbb{R}^D_{>0}\), and \(\odot\) denotes elementwise multiplication.

\subsection{Continuous-time latent dynamics}
Latent trajectories are generated by a neural ODE:
\begin{equation}
\frac{dz_b(t)}{dt} = f_\theta(z_b(t), t), \qquad z_b(t_1)=z_{0,b}.
\end{equation}
In \texttt{v5}, \(f_\theta\) is a mixture of experts:
\begin{equation}
f_\theta(z,t) = \sum_{e=1}^{E} \pi_e(z) f_e(z), \qquad
\pi(z)=\operatorname{softmax}(g(z)),
\end{equation}
with \(E=4\) latent experts by default and Dormand--Prince integration (\texttt{dopri5}).
Each expert \(f_e:\mathbb{R}^D\to\mathbb{R}^D\) is an MLP and \(\pi_e(z)\in[0,1]\) are gating weights satisfying \(\sum_e \pi_e(z)=1\).

\subsection{Decoder family}
A decoder maps latent states back to observations:
\begin{equation}
\hat{x}_b(t_\ell)=g_\psi(z_b(t_\ell)).
\end{equation}
The codebase supports MLP, neuron-aware, local-attention, and MoE decoders; \texttt{v5} default is MoE decoder with 8 decoder experts.
In all cases, \(g_\psi:\mathbb{R}^D\to\mathbb{R}^K\) outputs the mean of a factorized Gaussian observation model in PCA space.

\section{Training Objective and Regularization}
We optimize a variational objective with auxiliary regularizers. Under a Gaussian observation model \(p_\psi(x_b(t_\ell)\mid z_b(t_\ell))=\mathcal{N}(g_\psi(z_b(t_\ell)),\sigma^2 I)\) with fixed \(\sigma^2\), maximizing the ELBO corresponds (up to constants and a scale factor) to minimizing mean-squared reconstruction error plus a KL penalty.

The base objective combines reconstruction and KL terms:
\begin{equation}
\mathcal{L}_{\text{base}} = \mathcal{L}_{\text{rec}} + \beta\,\mathcal{L}_{\text{KL}},
\end{equation}
where
\begin{equation}
\mathcal{L}_{\text{rec}} = \frac{1}{B\,L\,K}\sum_{b=1}^{B}\sum_{\ell=1}^{L}\|\hat{x}_b(t_\ell)-x_b(t_\ell)\|_2^2,
\end{equation}
\begin{equation}
\mathcal{L}_{\text{KL}} = \frac{1}{B}\sum_b D_{\text{KL}}\big(q_\phi(z_{0,b}\mid x_b(t_1))\,\|\,\mathcal{N}(0,I)\big).
\end{equation}
Equivalently, the (negative) ELBO per trial is
\begin{equation}
\mathcal{L}_{\text{ELBO}} =
-\mathbb{E}_{q_\phi(z_{0,b}\mid x_b(t_1))}\Big[\sum_{\ell=1}^{L}\log p_\psi(x_b(t_\ell)\mid z_b(t_\ell))\Big]
+\beta\,D_{\mathrm{KL}}\big(q_\phi(z_{0,b}\mid x_b(t_1))\,\|\,p(z_{0,b})\big),
\end{equation}
with prior \(p(z_{0,b})=\mathcal{N}(0,I)\). In practice, the code uses a single Monte Carlo sample of \(z_{0,b}\) per trial and minibatch.

\paragraph{Smoothness regularization.}
\begin{equation}
\mathcal{L}_{\text{smooth}} = \frac{1}{B(L-1)D}\sum_{b,\ell}
\left\|\frac{z_b(t_{\ell+1})-z_b(t_\ell)}{t_{\ell+1}-t_\ell}\right\|_2^2.
\end{equation}

\paragraph{Transition-aware regularization (\texttt{v5}).}
\begin{equation}
\mathcal{L}_{\text{trans}} = \frac{1}{B(L-1)K}\sum_{b,\ell}
\left\|\big(\hat{x}_b(t_{\ell+1})-\hat{x}_b(t_\ell)\big)-\big(x_b(t_{\ell+1})-x_b(t_\ell)\big)\right\|_2^2.
\end{equation}
This term is linearly warmed up for the first 30 epochs.

\paragraph{Soft LLE latent regularization (\texttt{v5}).}
For flattened latent points \(\{z_i\}_{i=1}^M\subset\mathbb{R}^D\), with \(k\)-NN set \(\mathcal{N}_k(i)\), we add a soft locally linear embedding penalty \citep{saul2003}:
\begin{equation}
\mathcal{L}_{\text{LLE}} = \frac{1}{M}\sum_{i=1}^{M}
\left\|z_i-\sum_{j\in\mathcal{N}_k(i)}w_{ij}z_j\right\|_2^2,
\quad
w_{ij}\propto\exp\!\left(-\frac{\|z_i-z_j\|_2}{\tau}\right).
\end{equation}
Default parameters: \(k=8\), \(M\le 256\), \(\tau=0.1\).

\paragraph{Total loss.}
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{rec}} + \beta_t\mathcal{L}_{\text{KL}} +
\lambda_{\text{smooth}}\mathcal{L}_{\text{smooth}} +
\lambda_{\text{trans},t}\mathcal{L}_{\text{trans}} +
\lambda_{\text{LLE}}\mathcal{L}_{\text{LLE}}.
\end{equation}
The KL coefficient \(\beta_t\) is warmed up over 30 epochs to a final value \(\beta=0.02\).

\section{Experimental Protocol}
\subsection{Configurations}
Main \texttt{v5} settings from \texttt{config.txt}: latent dimension \(D=5\) (with a sweep to \(D=8\)), batch size 8, 150 epochs, Adam optimizer (learning rate 0.002, weight decay \(10^{-5}\)), \(\lambda_{\text{smooth}}\in\{5\times10^{-4},2\times10^{-4}\}\), \(\lambda_{\text{trans}}=0.01\) with a 30-epoch warmup, \(\lambda_{\text{LLE}}=0.01\), landmark count 100, and baseline correction enabled.

\paragraph{Implementation details.}
The encoder is an MLP with hidden widths 512--256--128. The latent vector field uses \(E=4\) experts with hidden width 128 and a learned gating network; derivatives are layer-normalized for stability. For reconstruction, the default \texttt{v5} decoder is a mixture-of-experts network with 8 decoder experts and hidden width 256. Latent dynamics are integrated with Dormand--Prince (\texttt{dopri5}) using tolerances \texttt{rtol=1e-3} and \texttt{atol=1e-4}. Gradients are clipped to max norm 1.0.

\subsection{Metrics}
The primary training metric is coefficient of determination,
\begin{equation}
R^2 = 1-\frac{\sum_{b,\ell}\|x_b(t_\ell)-\hat{x}_b(t_\ell)\|_2^2}{\sum_{b,\ell}\|x_b(t_\ell)-\bar{x}\|_2^2},
\end{equation}
where \(\bar{x}=\frac{1}{B\,L}\sum_{b,\ell} x_b(t_\ell)\) denotes the mean activity vector across all validation entries in the evaluation space.
Our implementation supports two evaluation styles: (i) PCA-space \(R^2\) and (ii) strict raw-neuron-space \(R^2\) via inverse PCA and de-normalization. We report values as saved in the run metadata for each experiment.

\section{Results}
\subsection{Synthetic benchmark: high ceiling with seed sensitivity}
Table~\ref{tab:synthetic} summarizes a five-seed sweep on synthetic data. The best seed reaches \(R^2=0.9789\). Excluding the divergent run, the mean performance is \(R^2=0.828\pm 0.141\) (std. dev.), indicating sensitivity to initialization even in a controlled setting.

\begin{table}[t]
\centering
\caption{Synthetic random-foraging benchmark from \texttt{seed\_sweep\_results.txt}.}
\label{tab:synthetic}
\begin{tabular}{lcc}
\toprule
Seed & Final \(R^2\) & Best validation loss \\
\midrule
1 & 0.9789 & 0.06642 \\
42 & 0.6757 & 0.40521 \\
1337 & 0.9116 & 0.16298 \\
2025 & 0.7467 & 0.31145 \\
777 & 0.0000 (NaN collapse) & $\infty$ \\
\midrule
Mean (all seeds) & 0.6626 & -- \\
Mean (non-collapsed seeds) & 0.8282 & -- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{E65 hippocampal data: mixed performance across variants}
Table~\ref{tab:e65} reports available E65 run metrics for the ODE-VAE family. An archived ODE-VAE run reports \(R^2=0.4368\). In contrast, \texttt{v5} sweep artifacts are substantially lower (best-epoch \(R^2\le 0.0569\), negative final \(R^2\) in three runs), while a no-PCA \texttt{v6} run records \(R^2=0.0956\). These discrepancies are consistent with the codebase supporting multiple evaluation spaces and split protocols; therefore, comparisons across runs should be interpreted cautiously unless the evaluation definition is matched.

\begin{table}[t]
\centering
\caption{E65 reconstruction metrics extracted from saved run artifacts. Sweep runs report both best-epoch and final \(R^2\) as logged during training.}
\label{tab:e65}
\begin{tabular}{lccccc}
\toprule
Variant & Space & \(D\) & \(\lambda_{\text{smooth}}\) & Best \(R^2\) & Final \(R^2\) \\
\midrule
archived ODE-VAE & PCA & -- & -- & -- & 0.4368 \\
\texttt{v5} sweep & PCA & 5 & \(5\times 10^{-4}\) & 0.0354 & -0.2391 \\
\texttt{v5} sweep & PCA & 5 & \(2\times 10^{-4}\) & 0.0353 & -0.1382 \\
\texttt{v5} sweep & PCA & 8 & \(5\times 10^{-4}\) & 0.0569 & -0.2022 \\
\texttt{v6} (no PCA) & raw & -- & -- & -- & 0.0956 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Manifold interpretability}
The codebase saves latent manifold projections (MDS) and reconstruction diagnostics for each run. Figure~\ref{fig:manifold} shows an example latent trajectory embedding from the trained model artifacts.

\begin{figure}[t]
\centering
\includegraphics[width=0.62\linewidth]{neuroscience/src/pt_files/latent_manifold_mds.png}
\caption{Latent manifold embedding produced by the ODE-VAE analysis pipeline.}
\label{fig:manifold}
\end{figure}

\section{Discussion}
The model captures the intended inductive bias: low-dimensional continuous latent trajectories with explicit geometric regularization. On synthetic data, this bias is highly effective. On real E65 recordings, however, results are sensitive to implementation and evaluation choices.

Three factors emerge from the saved run artifacts:
\begin{enumerate}
\item \textbf{Metric-space mismatch.} PCA-space training can look favorable while strict raw-space \(R^2\) may degrade.
\item \textbf{Data-efficiency tradeoff.} Landmark subsampling (100 selected sequences from 180 usable trials) accelerates training but may reduce generalization.
\item \textbf{Optimization stability.} Strong regularization with small validation sets (3 trials) and stiff latent dynamics can produce unstable or negative final \(R^2\), despite early high points.
\end{enumerate}

These observations suggest that future gains likely require protocol-level changes in addition to architectural changes: larger and randomized holdout splits, early stopping on a stable cross-validated objective, trial-level (not frame-level) landmark selection, and direct raw-space reconstruction losses.

\section{Limitations and Reproducibility}
This study is bounded by the available run artifacts and inherits version-specific logging differences. In particular, some run files report ``best'' and ``final'' \(R^2\) under different conditions, and not all checkpoints include identical metadata fields. We therefore report values exactly as saved in each artifact path. The implementation also exhibits training fragility (including occasional NaN divergence), which should be addressed before drawing definitive biological conclusions.

\section{Conclusion}
We presented a mathematically grounded latent Neural ODE-VAE framework for neural manifold modeling and analyzed a sequence of model variants (\texttt{v1--v6}), with \texttt{v5} as the primary model. The method can recover smooth low-dimensional dynamics and high synthetic reconstruction quality, but real-data performance remains sensitive to preprocessing and evaluation protocol. This work provides a formal foundation and concrete directions for improving robustness of ODE-VAE manifold modeling for neuroscience.

\paragraph{Societal impact.}
This work is basic research on neural representation learning from animal neuroscience data and has no immediate direct societal deployment. Potential long-term impact is improved scientific understanding of memory and cognition.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
